{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "eJrKcsdcAv8W",
    "outputId": "b9798a3a-89e8-4de9-c991-18c0791c4819"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m354.7/354.7 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.3/302.3 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.6/322.6 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.1/211.1 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m93.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.4/107.4 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m95.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m70.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m47.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m87.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for tinysegmenter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for feedfinder2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for jieba3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "!pip install -q -U langchain transformers bitsandbytes accelerate pypdf feedparser beautifulsoup4 gradio newspaper3k lxml_html_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "PDjg68hLh3z2",
    "outputId": "4f926bab-5702-45bd-ffb4-be9a3a89a31d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-community\n",
      "  Downloading langchain_community-0.3.22-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: langchain-core in /usr/local/lib/python3.11/dist-packages (0.3.55)\n",
      "Requirement already satisfied: langchain<1.0.0,>=0.3.24 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.24)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.40)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.2)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.15)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (9.1.2)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
      "  Downloading pydantic_settings-2.9.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.33)\n",
      "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
      "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (4.13.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (2.11.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.0)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core) (3.0.0)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.24->langchain-community) (0.3.8)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (3.10.16)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core) (2.33.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core) (0.4.0)\n",
      "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
      "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2025.1.31)\n",
      "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.2.1)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (4.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.0.8)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (0.14.0)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.3.1)\n",
      "Downloading langchain_community-0.3.22-py3-none-any.whl (2.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
      "Downloading pydantic_settings-2.9.1-py3-none-any.whl (44 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
      "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
      "Installing collected packages: python-dotenv, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain-community\n",
      "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain-community-0.3.22 marshmallow-3.26.1 mypy-extensions-1.1.0 pydantic-settings-2.9.1 python-dotenv-1.1.0 typing-inspect-0.9.0\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain-community langchain-core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "mh1LqWLji-qP",
    "outputId": "89a2aaff-5034-4ffe-dd8b-8ef7bc38bb7f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.11.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
      "Downloading faiss_cpu-1.11.0-cp311-cp311-manylinux_2_28_x86_64.whl (31.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.11.0\n"
     ]
    }
   ],
   "source": [
    "pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iFj_hnLaANoq",
    "outputId": "e4897347-5c8e-4b45-b1b3-41921fc61b80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gtts\n",
      "  Downloading gTTS-2.5.4-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from gtts) (2.32.3)\n",
      "Requirement already satisfied: click<8.2,>=7.1 in /usr/local/lib/python3.11/dist-packages (from gtts) (8.1.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->gtts) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->gtts) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->gtts) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->gtts) (2025.1.31)\n",
      "Downloading gTTS-2.5.4-py3-none-any.whl (29 kB)\n",
      "Installing collected packages: gtts\n",
      "Successfully installed gtts-2.5.4\n"
     ]
    }
   ],
   "source": [
    "pip install gtts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JkQtlFfYQyBR"
   },
   "source": [
    "multiple level sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "834b718e37164b6d839ac45ae576b5e5",
      "a0c45ed61521407db572051363b617b6",
      "283f86e963ff4aa9baa38816015febe4",
      "e8915c56ad4648f296637069aae4d8ee",
      "9d42ed9865a948619a8d1651feee9918",
      "9275c24748cf48c4838ca6abfc2bcdf5",
      "d4e6872d79fa4dbea5e43b18b315b7e8",
      "2343444660664ad39a2b574e1ff61e55",
      "7f819d5b45df4d90a65a200bde8bcc00",
      "d7013c784ee64ba58fb56d4ed01a220a",
      "79fd10f247be42c5b43d84cff5bd55f5",
      "2f485facd9d4487092938eb84f814ff3",
      "108ba111562544eeb18e2e4caf002d25",
      "9f5b3a37978d4c0eb655e588f9d66cb3",
      "edddfc9ae0bf4803b6401aa1a0e736f2",
      "3aecb668d0dc4c2c9241b46275cdcfe7",
      "9b05935397d448638525b5812ebcb3e7",
      "0e5526d62d4b4e06aeb3e05336068055",
      "e0081ce3b8474edca91d54e2bfd68b0b",
      "2e81f6b2970e444b85ea0d56281c195f",
      "0b87e33d97cf47939f32c87f17bf4fcf",
      "b7b5ec3aed2f4868a53100138d2ebc3a",
      "cafe2f8fa81c44d298cb211fc1dbcf42",
      "6b0d9a81998d43baa1c7dfe622636c36",
      "1f22f3584e074bde908d92cd0fa793d0",
      "a5a24dd2de964d58b782ba041f5bdb81",
      "662bf5e5fe8c4cf783ecf758d2bda201",
      "30b24ae7d7d74ee59828fbcbc61b2f6d",
      "96e153d48758443eb27ce244fe96e083",
      "659ea14cc3f547b09d862b5b9a9d0e80",
      "2b4b257d39024308b6049dd135965e91",
      "66633e669e6a473f9e8ae3982ebb0cbd",
      "af71a02d85e24057a99899c7fc16f108",
      "9a1e9b5fbb134a91906861e80fac7feb",
      "19bfe5f216244c878ebb466ae70235e7",
      "f21fa1b8f61d41a089e85a51397ed2b5",
      "3c64d618d2a14247a644fd1b36c6c715",
      "a1889918b90b4f72a7e309777ba2e56f",
      "abea666098f74800a7539df3a2db133b",
      "7abab81f538f44e2a89e71fa0c52dbad",
      "59d45a3b81ea4aa7ad4e59c469bbc726",
      "29f511b2c6d249bdaa92a276f026d30f",
      "67b2898b84104ad48016139c746dbc48",
      "bfb5c227b18140fd819950d4291d28b0",
      "4b755f87625843e58a59c54b85d0d650",
      "a7160f39eb364d9fb3890efd7a2d4e68",
      "61262f64480846b6a24577b42221df35",
      "a330123cb14d494d88989e3583da609a",
      "05173fdbf0964f5cab1c8a29c309d5b4",
      "ea2373f98c8e40979e4b5779c71d522b",
      "a979ee9190b54c2b80c4c0a8077350e2",
      "e98b045aed3d40dbbae79c8ee45ca9ca",
      "80e06980bb624f56ad03f84650c40c75",
      "b0c02284a4414b59887eed3775e902ff",
      "30390586097b421b8b694258255ac2a4",
      "c978668b9d0b48bbb5aa0ef3b5b65f18",
      "9de43b9578b4402593ddaacc2bed20af",
      "f22cbc0d4c0044a58b32a96d28d1991e",
      "1413bfa089704ed68b18a421fc5011f6",
      "9309cde65e8a46b089b852542c0e7b22",
      "e697e346dc3a4a679bf311d135a33bc4",
      "a4889c388f4f410fb3db921aae517203",
      "c733535443cb4eacb341ae0135a8d0b3",
      "1dea58794f434abe8d8f47de74d68311",
      "43129a41c74f4061abc46c2863bd3772",
      "8b62feb72d354faa9290a94f5b3a7ea4",
      "1c768144355a444a919280124d5b9736",
      "5340e8b0943a42499d505292fbab47f9",
      "28cb24aad35742c6ad8dbd8b043316ab",
      "4be0752445594271883f3b9c852913fe",
      "50dacf102f214111841140f08e8f8fd7",
      "2b2cdfb497f84478ac758c2f280e693c",
      "d5952901d4574c5cb0d58a84742eaf56",
      "23d49eaf63814c51bfba0f2283abf01c",
      "f691cad7bc8d45ecbe1085ecb1d13d8e",
      "4bbb1c20eb1542a081ece459d3f29a5d",
      "e7d6b8ffe8d3469e9275cca30321034f",
      "b77764c22a7b4e6e91642603094d2e50",
      "d40db65363fe4b73b4bec32f595eb5fb",
      "f5d22bd2ccb8461da52117fbc982038f",
      "c1564a31901b4f31af570550448df30a",
      "621f3616fab248429762514d8ca15ef5",
      "03b9aa0eac054d929555b5338f8fdd52",
      "70bb9ada57d7439dbd65fe607e8e56e0",
      "753e06a196f2489fa2c40d402fcf7a21",
      "b86e4fe96e054d4abbee0698622627bd",
      "0e576ceb090245d99c237846dc563b59",
      "12cb8bd746a747f496de4ee06620b637",
      "9e6e103440374ddb8b585e4870789b3b",
      "6c8a273e945142e4bacd188ebb194d5f",
      "bb6458605d2144a991797cf743bdc9b7",
      "2ed7f4dd96bf40c08f13f3f6b4e52ccd",
      "40bbf6d2ad014f4da8bf5fea939cf62c",
      "965ed0f67d77491190b7bf9986d24486",
      "38b4c4ff961a4c1c8a61018cae63a931",
      "1c55ce334c1042ce925bd8b6808c043d",
      "2a952a150a6849b89a7ccad9e35a4213",
      "80b1857064b44039825f4e175656edf5",
      "c6d773bcae704dc3a3bf81b4e018c98f",
      "3e55f052a1754448a7060500c34f8e8d",
      "6e5a005600ce4264a8ea004352db24f8",
      "74d51486b0e04f13bfaad5e60b08dbb2",
      "f1c8bd4b73564cfbbcaec56fedaab9ed",
      "791919dec1ec4ec1b1d835e054eaadeb",
      "c26bf04265024f5f9697ea5515cf5bad",
      "6733ff4669d843889e51dd94fe6ee028",
      "227dd57212e14c7eaf82aa53aa8d481e",
      "bd8baffd5cf64d92b11419193fde6625",
      "3fcfbc29c0d944d9bbb366ff511e5262",
      "848f6e2f3b5a4bd3990bf78b71fd6759",
      "302a62d5a55048f4bb206ed0824bb576",
      "214dc022ef794668a7019057279588db",
      "6151d8eb8b414d968edc6e405c13f234",
      "b598f12164194021ace51ab3082c44ca",
      "988f17706bf741e2817b8741556f0ecb",
      "e024fad7121a4b02a1a72f37d6f7c525",
      "381af17829a04ea796d342154e150b87",
      "19b85e539a1c478aa0760349e308d0f0",
      "381e05f6c15c4f63a295e845b78da994",
      "23d0d2ccdec04d5994054cdb9069b803",
      "71134f8884984a8a9bb0f720563bbc7e",
      "8dd1d30040e547bb9506c9409e9ab856",
      "c9b81aaf87cb452ca7355f2f83e67d17",
      "f40ed96a94e1417f9183a746dc17dca6",
      "3804c16b006b4c9290ce4829b693adfe",
      "3934ffe0dcaa46ed96a6d6cdb956b33c",
      "3d161fad56e94d64b2427f7f583d9c5c",
      "40a58d10f0e1469693929f833182de3d",
      "fc759b8259f34e439b750ab8bc075adb",
      "3a3502495d8f4c728d2ecffe4847bb1f",
      "d3388811e81b4d89b3bda8c7ae0290aa",
      "c7b3639cb10d4391a2dcd2b8ab01b4d7",
      "a020ca591479405d922c52b8b547e9b0",
      "d8fffcf9af0748afbe58425dadc78cde",
      "576caa40132e4fa882b1823b614bb137",
      "2a5bda95457f4966b8c044d1055d9c15",
      "230baa1ed46140ad86080c86139a6c57",
      "ed6d6e1962ea463c8e0228fd0a431695",
      "a12d3ddda4a2411dacf4202b4442304c",
      "cb7f5c2c01cf4b6e89752c71c8b71856",
      "3b7dd659a17b4e7c9f6b43440cc5f452",
      "78988fc5fed74135b578d0eb007034d3",
      "ec1d65f175e240769607428b2277190c",
      "7839518c1dec4e7db6425e8935242752",
      "cc0c8780b7294c3783a89e3fde0b6ff2",
      "d04f8f4d584d4c72b2a6dcee87f068b2",
      "b91544c0782c4fdbaf4370350b2247f0",
      "53717b9aa0cc41ff920b3f9ee7d28859",
      "b1ab825f2e69443c8ca8b7e0b72cca3f",
      "d0bfa63acb7b4f8b8ca3165936f18159",
      "a7ae3d5200b74a10b231acbe7e08c4cd",
      "6915506632ce4348923ee34a79e8f370",
      "927a0eb14bdb4a6db0572e215bb44065",
      "37699edf25cb42df9dbbe856c564b9d7",
      "258a372491fb4c34ac9a87a373f379d9",
      "58b11fd4a6b0477693f42d5a92466138",
      "5eb76cad7ec940b4b01b8fefbdbc6246",
      "74eb27906bca49d38030b2674994a199",
      "b11c1fdd5a904eb29d9bb151bba48717",
      "a99233a927f04c51b578135980741c42",
      "56e87b7595e94148a1c8d6daf49a8c87",
      "7f6261fc4d4445488e3fbaa236f86a24",
      "e38910561b19487d90164a13f773e1d9",
      "9affbc1b82a84762ab9bf37f125875fe",
      "510a253b11f04e03b8a96b8e6218c11b",
      "faa97bda299743d2b68b9b0a40d01595",
      "2cee43cb32d6464baef1dbaa2534ae65",
      "60fd530561b5429e93d02e9a94072771",
      "98b2852d49f54338b051c1dac29e7624",
      "37f26fc9916746069b31688ec4cf65ca",
      "a050fbbb84fd43a3904a1bb1b11a492d",
      "0d23b2f8884c4d64b74089e488ffa343",
      "6a3592ac164344c590949814ff55dff8",
      "16f5febeb7ad41619a1218edb73faed2",
      "99cf5a14820f4cab8d13f381907f6637",
      "8eb6674067364c62b05ea72d3660767a",
      "b35799213c544798ab07e35ba5d8f63b",
      "947dc764349b40a3adce756b00fff1a3",
      "29a13484333b44ddaf713006654af857",
      "6a79d8283afd4946b0d71d6fc55e3dec",
      "4b75c20eb45d4d33bb125ef11ff1bcc3",
      "47ba7d1922164a0196759c976c2f9e29",
      "b2a00772da834e53900e01a106e17cf7",
      "eeb55b02e7dd4cccbcb28b8f9e844137",
      "9970be3caa3a433994c230ca26ff4e33",
      "7a752680cb914166bcac891cdc251df1",
      "c3383dbe878c47228492455af1cc04b5"
     ]
    },
    "id": "kV1QSZf79yoM",
    "outputId": "494d39b8-b3e3-49e8-db66-2a21c7803cd8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "834b718e37164b6d839ac45ae576b5e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.58k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f485facd9d4487092938eb84f814ff3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cafe2f8fa81c44d298cb211fc1dbcf42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a1e9b5fbb134a91906861e80fac7feb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b755f87625843e58a59c54b85d0d650",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c978668b9d0b48bbb5aa0ef3b5b65f18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c768144355a444a919280124d5b9736",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/614 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b77764c22a7b4e6e91642603094d2e50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e6e103440374ddb8b585e4870789b3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e55f052a1754448a7060500c34f8e8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "302a62d5a55048f4bb206ed0824bb576",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dd1d30040e547bb9506c9409e9ab856",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a020ca591479405d922c52b8b547e9b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7839518c1dec4e7db6425e8935242752",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.62k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "258a372491fb4c34ac9a87a373f379d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faa97bda299743d2b68b9b0a40d01595",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b35799213c544798ab07e35ba5d8f63b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "<ipython-input-6-d35c315c90bf>:66: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
      "  llm = HuggingFacePipeline(pipeline=generate_text)\n",
      "<ipython-input-6-d35c315c90bf>:219: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chat_history = gr.Chatbot()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
      "\n",
      "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
      "* Running on public URL: https://7bff1b85e40ec8da94.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://7bff1b85e40ec8da94.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from transformers import pipeline\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from newspaper import Article\n",
    "import feedparser\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import torch\n",
    "import transformers\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from gtts import gTTS\n",
    "import os\n",
    "\n",
    "# ------------------- MODELS --------------------\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\", device=0)\n",
    "\n",
    "model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "hf_auth = \"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\"  # Replace with your actual token\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "bnb_config = transformers.BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    token=hf_auth\n",
    ")\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_id, token=hf_auth)\n",
    "\n",
    "stop_list = [\"\\nHuman:\", \"\\n```\\n\"]\n",
    "stop_token_ids = [torch.LongTensor(tokenizer(x)[\"input_ids\"]).to(device) for x in stop_list]\n",
    "\n",
    "class StopOnTokens(transformers.StoppingCriteria):\n",
    "    def __call__(self, input_ids, scores, **kwargs):\n",
    "        for stop_ids in stop_token_ids:\n",
    "            if torch.eq(input_ids[0][-len(stop_ids):], stop_ids).all():\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "stopping_criteria = transformers.StoppingCriteriaList([StopOnTokens()])\n",
    "\n",
    "generate_text = transformers.pipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=False,\n",
    "    task=\"text-generation\",\n",
    "    stopping_criteria=stopping_criteria,\n",
    "    max_new_tokens=300,\n",
    "    repetition_penalty=1.1\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=generate_text)\n",
    "# Fonction de résumé rapide\n",
    "def summarize_quick(text):\n",
    "    summary = summarizer(text, max_length=100, min_length=30, max_new_tokens=100, do_sample=False)\n",
    "    return summary[0][\"summary_text\"]\n",
    "\n",
    "# Fonction de résumé détaillé\n",
    "def summarize_detailed(text):\n",
    "    summary = summarizer(text, max_length=300, min_length=150, max_new_tokens=300, do_sample=False)\n",
    "    return summary[0][\"summary_text\"]\n",
    "\n",
    "# Fonction de résumé avec points clés\n",
    "def summarize_key_points(text):\n",
    "    summary = summarizer(text, max_length=200, min_length=100, max_new_tokens=200, do_sample=False)\n",
    "    summary_text = summary[0][\"summary_text\"]\n",
    "    key_points = summary_text.split(\". \")\n",
    "    points = [point.strip() + \".\" for point in key_points if point]\n",
    "    return \"\\n\".join(points)\n",
    "\n",
    "# ------------------- HELPERS --------------------\n",
    "def truncate_text(text, max_tokens=1024):\n",
    "    tokens = text.split()\n",
    "    return \" \".join(tokens[:max_tokens])\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "def clean_summary(summary):\n",
    "    soup = BeautifulSoup(summary, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "def summarize_text(text, summary_type=\"quick\"):\n",
    "    text = clean_text(text)\n",
    "    text = truncate_text(text)\n",
    "\n",
    "    if summary_type == \"quick\":\n",
    "        summary = summarizer(text, max_length=150, min_length=50, do_sample=False)\n",
    "        return summary[0][\"summary_text\"]\n",
    "    elif summary_type == \"detailed\":\n",
    "        summary = summarizer(text, max_length=400, min_length=200, do_sample=False)\n",
    "        return summary[0][\"summary_text\"]\n",
    "    elif summary_type == \"key_points\":\n",
    "        summary = summarizer(text, max_length=200, min_length=100, do_sample=False)\n",
    "        summary_text = summary[0][\"summary_text\"]\n",
    "        key_points = summary_text.split(\". \")\n",
    "        points = [point.strip() + \".\" for point in key_points if point]\n",
    "        return \"\\n\".join(points)\n",
    "    else:\n",
    "        summary = summarizer(text, max_length=150, min_length=50, do_sample=False)\n",
    "        return summary[0][\"summary_text\"]\n",
    "\n",
    "def load_pdf(file_path):\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    documents = loader.load()\n",
    "    return \" \".join([doc.page_content for doc in documents])\n",
    "\n",
    "def fetch_article_from_url(url):\n",
    "    article = Article(url)\n",
    "    article.download()\n",
    "    article.parse()\n",
    "    return article.text\n",
    "\n",
    "def fetch_articles_from_rss(url):\n",
    "    feed = feedparser.parse(url)\n",
    "    return [clean_summary(entry.summary) for entry in feed.entries]\n",
    "\n",
    "def text_to_speech(text):\n",
    "    audio_path = \"summary_audio.mp3\"\n",
    "    gTTS(text).save(audio_path)\n",
    "    return audio_path\n",
    "\n",
    "# ------------------- PROCESSING --------------------\n",
    "def process_input(choice, text_input, file_input, url_input, rss_input, summary_type):\n",
    "    if choice == \"1\":\n",
    "        content = text_input\n",
    "    elif choice == \"2\":\n",
    "        content = load_pdf(file_input.name)\n",
    "    elif choice == \"3\":\n",
    "        content = fetch_article_from_url(url_input)\n",
    "    elif choice == \"4\":\n",
    "        summaries = fetch_articles_from_rss(rss_input)\n",
    "        content = \"\\n\\n\".join(summaries)\n",
    "    else:\n",
    "        return \"\", None, None, None\n",
    "\n",
    "    summary = summarize_text(content, summary_type=summary_type)\n",
    "\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "    chunks = splitter.create_documents([content])\n",
    "\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-base-en-v1.5\", model_kwargs={\"device\": \"cuda\"})\n",
    "    vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "\n",
    "    prompt_template = PromptTemplate(\n",
    "        template=\"\"\"### Instruction:\n",
    "You are a helpful AI assistant. Use only the information in the context below to answer the user's question.\n",
    "If the answer is not found in the context, reply with: \"The article does not provide enough information.\"\n",
    "\n",
    "### Context:\n",
    "{context}\n",
    "\n",
    "### Question:\n",
    "{question}\n",
    "\n",
    "### Response:\"\"\",\n",
    "        input_variables=[\"context\", \"question\"]\n",
    "    )\n",
    "\n",
    "    rag_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        retriever=vectorstore.as_retriever(search_kwargs={\"k\": 2}),\n",
    "        chain_type=\"stuff\",\n",
    "        chain_type_kwargs={\"prompt\": prompt_template},\n",
    "        return_source_documents=True\n",
    "    )\n",
    "\n",
    "    audio = text_to_speech(summary)\n",
    "    return summary, rag_chain, content, audio\n",
    "\n",
    "# ------------------- GRADIO UI --------------------\n",
    "with gr.Blocks(title=\"Article + Chatbot + TTS\") as demo:\n",
    "    gr.Markdown(\"## 📰 Article Summarizer + 💬 Chatbot + 🔊 Text-to-Speech\")\n",
    "\n",
    "    with gr.Tab(\"Input\"):\n",
    "        choice = gr.Radio(\n",
    "            choices=[(\"1. Paste Text\", \"1\"),\n",
    "                     (\"2. Upload PDF\", \"2\"),\n",
    "                     (\"3. Article URL\", \"3\"),\n",
    "                     (\"4. RSS Feed\", \"4\")],\n",
    "            label=\"Content Source\", value=\"1\")\n",
    "\n",
    "        text_input = gr.Textbox(label=\"Paste your text here\", lines=10, visible=True)\n",
    "        file_input = gr.File(label=\"Upload a PDF\", visible=False)\n",
    "        url_input = gr.Textbox(label=\"URL\", visible=False)\n",
    "        rss_input = gr.Textbox(label=\"RSS URL\", visible=False)\n",
    "\n",
    "        # Ajout choix type de résumé\n",
    "        summary_choice = gr.Radio(\n",
    "            choices=[\"Résumé rapide\", \"Résumé détaillé\", \"Résumé avec points clés\"],\n",
    "            label=\"Choisissez le type de résumé\",\n",
    "            value=\"Résumé rapide\"\n",
    "        )\n",
    "\n",
    "        start_btn = gr.Button(\"Analyser l'article\")\n",
    "\n",
    "    with gr.Tab(\"Summary\"):\n",
    "        summary_output = gr.Textbox(label=\"Generated Summary\", lines=10)\n",
    "        audio_output = gr.Audio(label=\"Summary Audio\", type=\"filepath\")\n",
    "\n",
    "    with gr.Tab(\"Chatbot\"):\n",
    "        chat_history = gr.Chatbot()\n",
    "        chat_input = gr.Textbox(label=\"Ask a question about the article\")\n",
    "        chat_btn = gr.Button(\"Ask\")\n",
    "\n",
    "    state_chain = gr.State()\n",
    "\n",
    "    def toggle_visibility(choice):\n",
    "        return [\n",
    "            gr.update(visible=choice == \"1\"),\n",
    "            gr.update(visible=choice == \"2\"),\n",
    "            gr.update(visible=choice == \"3\"),\n",
    "            gr.update(visible=choice == \"4\")\n",
    "        ]\n",
    "\n",
    "    choice.change(toggle_visibility, inputs=choice,\n",
    "                  outputs=[text_input, file_input, url_input, rss_input])\n",
    "\n",
    "    def on_start(choice, text_input, file_input, url_input, rss_input, summary_choice):\n",
    "        # Convertir le résumé choisi en type attendu\n",
    "        SUMMARY_MAPPING = {\n",
    "            \"Résumé rapide\": \"quick\",\n",
    "            \"Résumé détaillé\": \"detailed\",\n",
    "            \"Résumé avec points clés\": \"key_points\"\n",
    "        }\n",
    "        summary_type = SUMMARY_MAPPING.get(summary_choice, \"quick\")\n",
    "\n",
    "        summary, chain, _, audio = process_input(choice, text_input, file_input, url_input, rss_input, summary_type)\n",
    "        return summary, chain, audio\n",
    "\n",
    "    start_btn.click(on_start,\n",
    "                    inputs=[choice, text_input, file_input, url_input, rss_input, summary_choice],\n",
    "                    outputs=[summary_output, state_chain, audio_output])\n",
    "\n",
    "    def on_chat(user_input, history, chain):\n",
    "        if not chain:\n",
    "            return history + [(user_input, \"Please load an article first.\")], \"\"\n",
    "        res = chain({\"query\": user_input})\n",
    "        answer = res[\"result\"].strip()\n",
    "        history.append((user_input, answer))\n",
    "        return history, \"\"\n",
    "\n",
    "    chat_btn.click(on_chat, inputs=[chat_input, chat_history, state_chain], outputs=[chat_history, chat_input])\n",
    "\n",
    "# ------------------- LAUNCH --------------------\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "87d45868c3814909bac98a4be1959a67",
      "d5b594712e824e288f789e041bc13977",
      "1103edafa6cd41b1a01142cc0df1d75b",
      "ce61afec4a0e447b82e0ab3b56775a06",
      "6da111e2806749e28ce9e07c051ec6df",
      "10026263e53d4a22bef0de468662dd4b",
      "1a23a262b19d43dba9274a3fc85bd99f",
      "e03635c2c7a04b68aa4c3de4ab9fa3db",
      "e3818d562d3b42c8a12d6fd0121a973b",
      "9ebd3eaa7e5a4e3ead7d8772b42509b1",
      "9e53256440d146198e0918aa3c88bb33",
      "4b14db8e423646e6a100cc2269f01345",
      "64ff844afce64e63ae1051e0aea3c70f",
      "e83d9462e22b4c2c96423c50cdaaf00f",
      "08de2a8e9d774905b474e25195167ef4",
      "41e842517d01465ba6f9c3dc5c9cec67",
      "b1be1e9c397e432aac742c3d9794877a",
      "cee03612a19241cbbb97072178768a92",
      "64928692019949f6b11529a87200325c",
      "f39b93943f3d4e01b5fdf92f27fac647",
      "8d76e7ddb66d471f969a2da45f049bd8",
      "5fdd3ca1f372486aa663ae0cbfe2107c",
      "f3b88b0bd9724a9b8f08d6d1a93e5d6c",
      "bbe7b538b4e24a33925fbdfb55c26c2d",
      "8637477ffca94495aa82aa4974a4a544",
      "5a79d8f52acb494ea8e74375f026d382",
      "ce4a192265c645df90d053f7652a775d",
      "4571f6b2c2c14790995232143212d1ca",
      "70b6a20bd5e34a54971f37e8e1ab5547",
      "360ca2a2161a4cdeb6c8354ac48d1a78",
      "90d39b1c0dfb42c889de2631a7a0fabb",
      "131b625364984c62a72a2621eed61ae0",
      "236a7c6106724eeca550fe93c08c8e21",
      "1d113d8908b84296937a8e31457adf03",
      "1409546227654f0baf305f434cdc8022",
      "6d0c80976782457b97b9bc01b7e628ab",
      "48fcff8200be449bb685b29fba0a0e49",
      "ba14413e6a3045ce84a61f6fd5fd8f20",
      "72d44d669f1140df97493d0367cab0e0",
      "b60a63ffdbdd43068bf19299e96d6534",
      "02393cae8f794e9fb8f79e7b17ad9291",
      "e3428e1c6c8e4fac98d1256dee3d65fb",
      "55458d5690c8485ea2eb57c7f5aa73dc",
      "cb284c3fe6234b848946ca6aa9718c09",
      "8224b74a762d48c299397e410752f6e6",
      "fc6a3c8955434987b42d1a43d2b14cd4",
      "2312e340bf8746968e29b8292e3c1a8d",
      "bdd5fd71789844a98200137bc7bb6fcb",
      "290423fc2916411eb404672fe16492a1",
      "fba0cb3e65624010889f30ab14eb89db",
      "76a1e77171444d53a6202d8d5ca7418a",
      "b6006b05cc1c4c35b90e585ff4a5097d",
      "b52bb0375c6d4f379fd431fbcffc5fce",
      "8e75ae784fb549218867596ef039a18f",
      "ec68689f302f4cf29a6d5d4345f55e86",
      "18459df64ecc49fc9acbabd5c07500c4",
      "b879281506ab4f509d3c3af8fad19368",
      "fc2c95db2ed14d7b8f26c3df5264d011",
      "89de9702afe54e01a47845fdec91c81c",
      "1e5f69c5f6b241ae95b955210e009d3a",
      "f1b403e4d83a41719151381209865245",
      "6c25b5cd04894317a421789b535906f6",
      "e5f1ed6b5c12465abb97bf0043ec746d",
      "9a2ac64532304d9eb0857b1ee78122be",
      "adf5f018ebd540a3999142f57a454643",
      "aab27c3d8c1f41a68d5122feecb4995a",
      "b5e4c8a4bb294ce69d204922b30aa08f",
      "9966da600e5a4dc48c100c93f1bd1da7",
      "fb1689b3fb834cc6a513fcd5bbfdaa9b",
      "8a1bac3706e44296848188a2f5ad17dc",
      "c265c7c7fa034ed792e45ee503080f88",
      "219eff273fb7401e8bfd0ba5d3b98401",
      "68e1f48f886649c398e99d8161a953c7",
      "7f25f55c4c174aabbfd72f1eb75fa546",
      "be8759394fc349699380bb45c077b83a",
      "96d098cb10864a63a7822485c34bf522",
      "8bde7bbce7234e039ef7f747d9017ff9",
      "c62e16fac10a4c11946b064ca5685003",
      "bd88258a659c4d959e2d4280fb535a95",
      "5796237d20044a76a951c2dff8fd8457",
      "502d623490134edcb66d5f05746b0571",
      "3e4bc8180dec409bb0afa8cc52176100",
      "47be6fc9111c4f56b729b6c21b2cea48",
      "3dace1ae538e45c2a5ae698fcc83962d",
      "7dd53f5686e147fab57d3add8a64aca0",
      "0406fdcda2e04b0381ffa944e474d6c6",
      "9aa84413a2784b1ca0f350319e3f034c",
      "4d0ac3d06a0d4aa5901a369a4e8a439d",
      "b94a31f71d3e414d8e455fec8d67e5e4",
      "79cf2032712645a6b2514cc66c23e357",
      "0669fff841c347e7b532a04d12b640c5",
      "8a82ca5c3d904af5be41eabc079ba16e",
      "4791161d5fc846cc8ab03c21ad02fb27",
      "caa00d5ff68b44c88cdd85a25d84e30c",
      "c43f9e4f62a9453982b48c9a2862ec85",
      "3c7f788c7df445cc8513a75cfefb9bad",
      "65813877c40f436bb821dedc8352d1ef",
      "4f3922de058043879da383cc6b86ca71",
      "b714e9bf4e1c43db80887e151aa5d42d",
      "4b5b5b1f63fb400ab90b5a03a4c6278d",
      "b142f43cecdc4887bae45f430f587413",
      "15240598fa0a40e2aad844abb2da9f93",
      "ecd8639713c647e68bb0214d0592a12d",
      "97d9dac4193443c5b1365bed46b4718a",
      "a1bf90fc9e594a0cadcb89e63951ea5a",
      "38774f61eb5c44ebae29e7e9d45a68be",
      "0f8681808e6345689d9050b4065d155e",
      "514e32d4cdd94e1a816d48175fa6c35e",
      "43dc7b6af0df495ba6dd889c3017f3c6",
      "9c1e4cbf749b4e408b07c38844130c4b",
      "10177280de5748a3b99e39ad0d3326dd",
      "f19963138b0e4b62806d1ccff78059e3",
      "f980e442dae44d7fb655c89877e8e832",
      "b3e19b6f6c974f74917f7be7bb53c7e4",
      "9ec94570a2e442d2bbad2d6a7607fdbb",
      "8dd03c67bfa54c499b5bafbff79654df",
      "5e6c002f193e487ea069836121805841",
      "8b8f53afc22f41cd98b1da7ad8a181c7",
      "7b3321684e7c47debb4737e564b7c18e",
      "5fcf2c42bf5743438fba3e3fd2362314",
      "6796190811bd4f5eb28b3defebc1edf6",
      "de5072b069754771be866c0a834efc7c",
      "0292a733c6cf449bbfd3738f944be1e7",
      "abea19b805a445a7ac17b3d425ca0496",
      "62dfb235453b409ba45cf91279cbc7f9",
      "f9b92c33937147c78c9bca1af2d8f9db",
      "aaaacc879d084c98910540f381199a90",
      "aabfab470f4c476295b131dfbe9933c8",
      "b88062188c8444a989b5602bbb39c75d",
      "6f91d6c39a1643c0bb757079000267d1",
      "6929a21962e8411ab3be911acde19e74",
      "6d9fc36aae744e4f98cc12fb2aa43285",
      "601905cee0554075b3a14725fd0bfeb6",
      "364db9f2367e471bbcc8fdc02f998a04",
      "982e7a33243b43d39e9b5fe7a1252f2a",
      "10b45140418b4a359bdf3d0994b9f463",
      "2220de4ce56a4dc7bf4d482abdc09bdb",
      "50f63aefe19049a997c487ecd2cc1f7e",
      "9caca8e09bc24671a27ac4fc76fc1bd5",
      "86764c3bdf4f4ba4aa43da6807ef0131",
      "e5461e60fca1490da6087f05e3ffeebc",
      "7e6f34eb11784ffda8c9c1ecba1c7863",
      "98083acf902c48e7b6cd721920dddae5",
      "6c38eb9cd6dc4d929ca54df9a51036f5",
      "74bf8ac38d9041a2bcf861a6d4b17ed5",
      "4257968a57154746aa16f9c672c0992e",
      "0c36ea30a325439f97516e9eaa2998ff",
      "113b7dc44246473e9ae20860a358bb97",
      "55c5a86544514702919b63fcd840f1de",
      "e5c51f85ca6148b39eea33d8d622174e",
      "33e16c283da64627893ec5b50e2ec852",
      "543c5ee8d9da4f1996c8e8e1b9323fad",
      "f280713e06e44708b7a58445d5f64b94",
      "028dc5943d7e48b984ace748cb0b6260",
      "f7eea669c39942719d26da762900c93f",
      "d494b59a8fb544f79f7c77692553e5a4",
      "effe7e6ca5c14791b3319d6409855a33",
      "ac7e25aae78e41648f515be708c1f361",
      "04a452f2200645fda86defaba04a8b7e",
      "e19ff555cbf149a988b231ff139a2fe4",
      "c3e368dab4f84cd0a9ff0358ccd224a9",
      "68f00a4cc780451bb106786818d59cc4",
      "9b204c261a784b4fa7919b169e9b61c4",
      "9eb608eaaa6142639a50996c7db3b627",
      "77d9a55c64ac4ab6b62f836864cda3e4",
      "893fff105ebf4e21af8a2f45f37841d1",
      "4a6dbf24e6eb43e6af7a3259297afe87",
      "190076083e2f428281485726ec50e7e0",
      "0dbd8983c08841cb806f087a38876f83",
      "d3564f520c1c45578839c1df37b9d271",
      "05b7ef00419b43629eb9bb0fa23ad044",
      "b1da3d71ccb24521b225b2cbd2a2773a",
      "b4a7e30c27ac4d058b3e129e8e701482",
      "26c3996d6e8f4499a6af51180efa9b57",
      "6fd494c914c94874a20da0d81cec152c",
      "319f5b821f0c4ae7ab46074b89e0e6bb",
      "4ed7a504742d453f995e99c82f6be4e4",
      "23a9bc2a2d824a7391f2eba927bb11c1",
      "8d247e39d7fe44b6b44de5c97569741a",
      "8a613b0bc17b47578e314e1bdce3abb4",
      "8241a1f44fec4b289a555c3eab85f9e0",
      "6d87eea4bf35470985e7701e71a4410a",
      "c84692da02d1451888fb852bf775fee6",
      "1bd96bc783e04192ab0e59429858f65f",
      "2284141b942a42e49800041213d66216",
      "f40b708cdb5f4927af4aeb575c398115",
      "70715891572641a3bc1fa13e2b5b68a3"
     ]
    },
    "id": "iz5Fqh5e_9vb",
    "outputId": "13aa4c7a-8b7f-4d3e-e003-ea2dece82cc4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87d45868c3814909bac98a4be1959a67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.58k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b14db8e423646e6a100cc2269f01345",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3b88b0bd9724a9b8f08d6d1a93e5d6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d113d8908b84296937a8e31457adf03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8224b74a762d48c299397e410752f6e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18459df64ecc49fc9acbabd5c07500c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5e4c8a4bb294ce69d204922b30aa08f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/614 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c62e16fac10a4c11946b064ca5685003",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b94a31f71d3e414d8e455fec8d67e5e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b5b5b1f63fb400ab90b5a03a4c6278d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10177280de5748a3b99e39ad0d3326dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de5072b069754771be866c0a834efc7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "601905cee0554075b3a14725fd0bfeb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c38eb9cd6dc4d929ca54df9a51036f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.62k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7eea669c39942719d26da762900c93f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "893fff105ebf4e21af8a2f45f37841d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ed7a504742d453f995e99c82f6be4e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "<ipython-input-5-22e2277cf5ac>:66: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
      "  llm = HuggingFacePipeline(pipeline=generate_text)\n",
      "<ipython-input-5-22e2277cf5ac>:179: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chat_history = gr.Chatbot()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
      "\n",
      "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
      "* Running on public URL: https://5599f81d37a126b040.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://5599f81d37a126b040.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from transformers import pipeline\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from newspaper import Article\n",
    "import feedparser\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import torch\n",
    "import transformers\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from gtts import gTTS\n",
    "import os\n",
    "\n",
    "# ------------------- MODELS --------------------\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\", device=0)\n",
    "\n",
    "model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "hf_auth = \"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\"  # Replace with your actual token\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "bnb_config = transformers.BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    token=hf_auth\n",
    ")\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_id, token=hf_auth)\n",
    "\n",
    "stop_list = [\"\\nHuman:\", \"\\n```\\n\"]\n",
    "stop_token_ids = [torch.LongTensor(tokenizer(x)[\"input_ids\"]).to(device) for x in stop_list]\n",
    "\n",
    "class StopOnTokens(transformers.StoppingCriteria):\n",
    "    def __call__(self, input_ids, scores, **kwargs):\n",
    "        for stop_ids in stop_token_ids:\n",
    "            if torch.eq(input_ids[0][-len(stop_ids):], stop_ids).all():\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "stopping_criteria = transformers.StoppingCriteriaList([StopOnTokens()])\n",
    "\n",
    "generate_text = transformers.pipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=False,\n",
    "    task=\"text-generation\",\n",
    "    stopping_criteria=stopping_criteria,\n",
    "    max_new_tokens=300,\n",
    "    repetition_penalty=1.1\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=generate_text)\n",
    "\n",
    "# ------------------- HELPERS --------------------\n",
    "def truncate_text(text, max_tokens=1024):\n",
    "    tokens = text.split()\n",
    "    return \" \".join(tokens[:max_tokens])\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "def clean_summary(summary):\n",
    "    soup = BeautifulSoup(summary, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "def summarize_text(text):\n",
    "    text = clean_text(text)\n",
    "    text = truncate_text(text)\n",
    "    summary = summarizer(text, max_length=200, min_length=50, max_new_tokens=150, do_sample=False)\n",
    "    return summary[0][\"summary_text\"]\n",
    "\n",
    "def load_pdf(file_path):\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    documents = loader.load()\n",
    "    return \" \".join([doc.page_content for doc in documents])\n",
    "\n",
    "def fetch_article_from_url(url):\n",
    "    article = Article(url)\n",
    "    article.download()\n",
    "    article.parse()\n",
    "    return article.text\n",
    "\n",
    "def fetch_articles_from_rss(url):\n",
    "    feed = feedparser.parse(url)\n",
    "    return [clean_summary(entry.summary) for entry in feed.entries]\n",
    "\n",
    "def text_to_speech(text):\n",
    "    audio_path = \"summary_audio.mp3\"\n",
    "    gTTS(text).save(audio_path)\n",
    "    return audio_path\n",
    "\n",
    "# ------------------- PROCESSING --------------------\n",
    "def process_input(choice, text_input, file_input, url_input, rss_input):\n",
    "    if choice == \"1\":\n",
    "        content = text_input\n",
    "    elif choice == \"2\":\n",
    "        content = load_pdf(file_input.name)\n",
    "    elif choice == \"3\":\n",
    "        content = fetch_article_from_url(url_input)\n",
    "    elif choice == \"4\":\n",
    "        summaries = fetch_articles_from_rss(rss_input)\n",
    "        content = \"\\n\\n\".join(summaries)\n",
    "    else:\n",
    "        return \"\", None, None, None\n",
    "\n",
    "    summary = summarize_text(content)\n",
    "\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "    chunks = splitter.create_documents([content])\n",
    "\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-base-en-v1.5\", model_kwargs={\"device\": \"cuda\"})\n",
    "    vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "\n",
    "    prompt_template = PromptTemplate(\n",
    "    template=\"\"\"<s>[INST] <<SYS>>\n",
    "You are a helpful assistant. Use only the context to answer the question.\n",
    "If the answer is not in the context, say: \"The article does not provide enough information.\"\n",
    "<</SYS>>\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "[/INST]\"\"\",\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "    rag_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        retriever=vectorstore.as_retriever(search_kwargs={\"k\": 2}),\n",
    "        chain_type=\"stuff\",\n",
    "        chain_type_kwargs={\"prompt\": prompt_template},\n",
    "        return_source_documents=True\n",
    "    )\n",
    "\n",
    "    audio = text_to_speech(summary)\n",
    "    return summary, rag_chain, content, audio\n",
    "\n",
    "# ------------------- GRADIO UI --------------------\n",
    "with gr.Blocks(title=\"Article + Chatbot + TTS\") as demo:\n",
    "    gr.Markdown(\"## 📰 Article Summarizer + 💬 Chatbot + 🔊 Text-to-Speech\")\n",
    "\n",
    "    with gr.Tab(\"Input\"):\n",
    "        choice = gr.Radio(\n",
    "            choices=[(\"1. Paste Text\", \"1\"),\n",
    "                     (\"2. Upload PDF\", \"2\"),\n",
    "                     (\"3. Article URL\", \"3\"),\n",
    "                     (\"4. RSS Feed\", \"4\")],\n",
    "            label=\"Content Source\", value=\"1\")\n",
    "\n",
    "        text_input = gr.Textbox(label=\"Paste your text here\", lines=10, visible=True)\n",
    "        file_input = gr.File(label=\"Upload a PDF\", visible=False)\n",
    "        url_input = gr.Textbox(label=\"URL\", visible=False)\n",
    "        rss_input = gr.Textbox(label=\"RSS URL\", visible=False)\n",
    "        start_btn = gr.Button(\"Analyze Article\")\n",
    "\n",
    "    with gr.Tab(\"Summary\"):\n",
    "        summary_output = gr.Textbox(label=\"Generated Summary\", lines=10)\n",
    "        audio_output = gr.Audio(label=\"Summary Audio\", type=\"filepath\")\n",
    "\n",
    "    with gr.Tab(\"Chatbot\"):\n",
    "        chat_history = gr.Chatbot()\n",
    "        chat_input = gr.Textbox(label=\"Ask a question about the article\")\n",
    "        chat_btn = gr.Button(\"Ask\")\n",
    "\n",
    "    state_chain = gr.State()\n",
    "\n",
    "    def toggle_visibility(choice):\n",
    "        return [\n",
    "            gr.update(visible=choice == \"1\"),\n",
    "            gr.update(visible=choice == \"2\"),\n",
    "            gr.update(visible=choice == \"3\"),\n",
    "            gr.update(visible=choice == \"4\")\n",
    "        ]\n",
    "\n",
    "    choice.change(toggle_visibility, inputs=choice,\n",
    "                  outputs=[text_input, file_input, url_input, rss_input])\n",
    "\n",
    "    def on_start(choice, text_input, file_input, url_input, rss_input):\n",
    "        summary, chain, _, audio = process_input(choice, text_input, file_input, url_input, rss_input)\n",
    "        return summary, chain, audio\n",
    "\n",
    "    start_btn.click(on_start,\n",
    "                    inputs=[choice, text_input, file_input, url_input, rss_input],\n",
    "                    outputs=[summary_output, state_chain, audio_output])\n",
    "\n",
    "    def on_chat(user_input, history, chain):\n",
    "        if not chain:\n",
    "            return history + [(user_input, \"Please load an article first.\")], \"\"\n",
    "        res = chain({\"query\": user_input})\n",
    "        answer = res[\"result\"].strip()\n",
    "        history.append((user_input, answer))\n",
    "        return history, \"\"\n",
    "\n",
    "    chat_btn.click(on_chat, inputs=[chat_input, chat_history, state_chain], outputs=[chat_history, chat_input])\n",
    "\n",
    "# ------------------- LAUNCH --------------------\n",
    "demo.launch()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PxPDSTunfaIC"
   },
   "source": [
    "chatbot seul avec RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "783f934afc62497ab988eaea82aca007",
      "da04b600e26e4b4eb98ef7cd14381027",
      "639b0b775aeb4b05811c92390c17e8f8",
      "69aa12ea274542d785c7817f06c247aa",
      "899570547290406f85184e6cbb190429",
      "b76946bcf09c47688cc0f91c70a2f6a8",
      "024a343ee23d45fc827741af9079d143",
      "ba94aab3443a4ba5afa97f59e87fa649",
      "862fa79f66d34400ac6684632fbc9c06",
      "2deffa25df184b6c968c983ad8017577",
      "448f7349619c4f4a938288b4c457715e",
      "ea88f60c993e490d93770b315c3a88fb",
      "b0f91a9ff4d449e7882cf9a3c7a6212e",
      "9f19ee67a12944b698a0b99762db9c18",
      "d52ef84d127c407198f4b83a5aa72231",
      "2f307cbd146d4c62a3716325c4d3d20d",
      "da3f552923fa45498621b08bbfd0ae63",
      "3a29df8765f64c7b8af65db830b22042",
      "02aeded2d422456fbc8d6520b68b5f75",
      "426774ad719241e7adaa43cb06d53bd7",
      "64d2ff8f8d5c4fa8bcc46c84f0969ebf",
      "803faa28bf5b428c9969cda89fc59b95",
      "a06c787e2142499daa5ee26f7dfcc66f",
      "1c82bca78b204a56b494aa594c7b1393",
      "4fb396826060415d9273ade986e588a6",
      "07cd0ad2eaf447ffa8ecbceae60bc754",
      "a93cabe4903b478f80e1cc5f7c1582f3",
      "ea49f6b1742c429aa06bb5a64899b476",
      "ce3566b61c6a48b3a1c10b9b94bc9856",
      "0246bf1364454b26ad08dec23ae2a94c",
      "51cb9112e28a44bf8a59e945fe334c79",
      "e700c0206ec042f8bb0b06dbed07ae35",
      "0d7d9c5f73874c3897a9909ca8564827",
      "eea5b8e1ebcf4b0b85e1141d95608b4a",
      "5f817f610cfe4ffeb6231eaabbb734e8",
      "b593902f8f6444348794f6465be74ebb",
      "0fc162e2f1f6445dbb1bd1313493a812",
      "ea7325d70081438a936e6d59e078d07a",
      "0d90706aa8364efa9ff0fb3d0c99282b",
      "5df2526921f248898f57076e20faeef1",
      "08dc00be6e0349e99cbe58e7be256729",
      "708d3752716446e8b75289ef7545e58e",
      "5e590ac2d5e6482ab608e7f6a0e5a7f2",
      "b88ee028ff804d16bcd1017201c4b857",
      "7df0f0b1c60c4013b3e171aabce7f13a",
      "49244b4f0554477c956f1879fb8757a9",
      "450121ba5646436b8e2e2756499afe68",
      "aa5c73309d50400b93973ee08fcad06d",
      "17f7568b9c454955a82f6ea24b9e4eaf",
      "bfdea0909640479a90ce779fd88b0e64",
      "d5173784de2d40acb53e982a1e14f117",
      "6da538f5e9a042da9922d2e91a70df5a",
      "7d8b4490e5d0443b9294e19ca13760da",
      "6e4560859c7147c099fc0278c25403d6",
      "0eebfda7e9734c25891f5b5c59269565",
      "bf4afc8af1a94e9c977f187f03b28a6a",
      "594be84973bd40b7abcb98845b19c491",
      "cb5f4c7a415146caa88824305d01e3b7",
      "8d5a14e8c0394cefad42c00ab1114ecc",
      "51701c122a56442ba93fc0310cd02a04",
      "3d6e3630e4434787962dd75a1fd0817f",
      "33d58e378177449486a12e55e6125c2a",
      "8fba5a7bf3ab4345b79261abb62bcc63",
      "c1e55ba3645f495d894cbbff42f0a8b4",
      "2953d35675244d218c5dec7abd946e50",
      "b65e2eb748dd455e992c7bd7d5fb8acd",
      "1a233654a3ff45d0bb119e4f9194517a",
      "5d562002cdee44f58f26bc41c3cc75ec",
      "925ac93dea644c57a54ece19083eff74",
      "91d97db847ad441f8ead7a4dfcd2cef2",
      "f46310751e804631acfe3539bd8031e4",
      "f66f3b43af214f0596ba7acc17c0ebc1",
      "a292e51bba5741ed807ebf7cdb4e93aa",
      "95eada02858c46bc954f3ad67282e943",
      "7ef55cac4e2643ca80061e48b3c206a6",
      "1d3424a576924fe4afb28a7321e20970",
      "08e083c8954c46768d559d5ee4c765a4",
      "bd405a7e80b94d3487f0884839ed9078",
      "a7573ad9fa664f8c8588bea4b7a9f087",
      "8556aee5f8f84c7782774c3a2be180b9",
      "682883e6704145c581c73c1dc0605820",
      "0388cfb181824fe399fe67f018e2dd47",
      "8a7bbed55bae4eafadc2319e674c098d",
      "99624f41560e4bf19110315fdae692dd",
      "5285934f3b71428aaad5ecc8666309ca",
      "97849223bcd1499a83145fbc4d2321e9",
      "61ace93adff44a92aeca0de60b0b195a",
      "63b76dcd5b7c46f89b8f184b72475cbf",
      "6f095a60bfb64c00ac3866117283567e",
      "fda4577b23ff4e9fb4c2d17a54e0b708",
      "db2855039dc3498c8580563ea1eeeda4",
      "68780983c3f54fec8add427ef95ac524",
      "d7fff7d1619a45828ce136154411f016",
      "0a48df19cebb4e0da8a07081f21e2953",
      "8d5eae347ffe439b85818e2a68ade430",
      "33f9a9ab6ea344c59e820ce776dfc628",
      "399f604220864e8b95d8429faf41ecb8",
      "0cf17c4daa4d4a199b48c9563116797b",
      "942b4e3fdcd64f9f9236f3ef3e216455",
      "45463f1c44e14bdea35970c8ad65f57e",
      "939d91a03a574c198de2524d3e7bde90",
      "cba10551590e414e92dd5eba13bd53f2",
      "5011b49ffe2a455f982e7fbc5f5717ea",
      "ed7b8e87198f4e6a8cac4cd011e42c4d",
      "06f63ae8298e467cbcf37a7dce8cbfa2",
      "6210f961d22b4ec3a9c193c0884dddfb",
      "ff3a6f3b1fba4fd0bbb9694f98372120",
      "da7c78d10f5e44f199fcc0e80e8bb028",
      "6eec740c7e534ec08903b4aabb13777d",
      "af2382ad4fd54010980cf86693bae04d",
      "3fb801b293f54e57acc125ef9fe9231e",
      "dc7fef7d7587469c95f96fe08af672da",
      "d0613b4676604ad2bba470dc38d6321d",
      "6fae05b95b124aa392288e950844bb64",
      "00cb55d765e2457c9c8820ff2d3c22de",
      "93168a85920046b48bfd6a06c10d7a1c",
      "a71c33aec3bd4933bb25852799baa548",
      "1128629c3afe4aa181d56e999bb05fab",
      "a1e4988f39aa4231be76804a5feae70a",
      "2020d02194eb4f6e83e1fe6b6a593af5",
      "9aa5d7e553ba41479962743806d8bf41",
      "c4b863b60e05432a9576a1cff78c58cd",
      "18c8a3d9061c43b7891e1c21aa2cdb5f",
      "23b6d4606ac9476f8058cc9d6746c699",
      "c0a23e1803a14b56b70163f3e10b55fc",
      "6829994d198843fe8a6c0ed454dfe617",
      "1ae41089de84411cab5e7585afea895c",
      "0a3778554b194780bcf014c3514b828f",
      "b77ae012b7a84ee79019398483ce17e7",
      "e085b2dc3efe4f89ae7037f16818071b",
      "b28e57a50eea40819d5e47a208c1b048",
      "3245d31346874422aedf216b614f720f",
      "64783898633b4c9392700c21e8c5240d",
      "8e6a291ca3ba4dbf848aff3bd4beb56e",
      "8f0c1093d77242399bdda93d30697621",
      "b0c6cd6afbff4e6d8e105353dd7d21b3",
      "0257aca524864c5daedb111725b79f2e",
      "ae5173c7d4894a9584bee4c91de00b4c",
      "3747bebf1ec845819da8b419cd2681a7",
      "c3bbe744de004f27adc3c31336433625",
      "c4cd964865b649cc963afdfea60f2f21",
      "52bfa19b5d1c44b790f0e89a48e958ed",
      "e1c9f7bbb1e04ffab2bf8fd2fad48b99",
      "5065a5bf61934180a4cab4ec30139910",
      "913a5962536247019d9d3471e2a535cf",
      "c1f90b5fbd2740579d2633567da6e306",
      "c56be63628ae4d989bffd516cc866aeb",
      "7b1b67a5814e4bcf955a78e50ea1c877",
      "0cd5fad747284024befa3333b35939ff",
      "d5922b28da3240b59f589a384f0e0a81",
      "52e739fb1da84519b8ec916a361a0157",
      "212c3ec95f9c4508a65fbdcd54f05015",
      "82bd352f2ef540199590a5845d2562a4",
      "30429953b4184f58894f3e17e92ea313",
      "b904557d5da94906ac33e1ef8db077d0",
      "bc0b791531824a55950645d5de4071d7",
      "84a8a6ec214041c7ae928b526ab9c0e3",
      "98106939e33a4a848da3a4541560c99e",
      "32f34dc3b4b54e939c83bddf0dc396ae",
      "f625599564f64fbcaad4cc77a1929c10",
      "312f21fbb9db46efa5459934deb13294",
      "0453130951f14ebf924f0e9ff8d14a71",
      "e1e158cb097145d9b1d2cb484f00b8f9",
      "d30bc9fa365a48d98d6181d371790dd2",
      "13137c59107247d9af4d39088c568a6f",
      "a72b129f0c824b66923fa1baa57794d4",
      "dd4807a719bf4791b89291344b339446",
      "4217b65993c345f69a6f08cfe8238e12",
      "bc48e1fadde645fc8cacda5c3d6660cc",
      "c952eedfc75f4fc6bb8bb9eeec84f2c6",
      "63a1b132d46e41149d3f2f54fbfcc8c1",
      "27fb258855604a4583200d5764bdf09a",
      "246c001b6c5646b3ab2525fa2b47ec77",
      "7410db5b7bf2485581f1b8b8940a76b0",
      "a468a97503504a3689027dcb9fc6475c",
      "ce24a056ffba44b680ab4d8828115da3",
      "d9aafaed66f740c2a96f0ab763e48e4c",
      "8082c979c49645b8a7a9d62955bf8a71",
      "bc3bf0ecfacf42bb9b93c9677a6f61c1",
      "186062453be74d5e896ac76f6b6ef42d",
      "c135b47ec2a1465ba19fcb6d7b350c7c",
      "c3c4547d56fb44e79b327bac03a2b9f3",
      "69fa363caa2540f7a40052336ee1d8cb",
      "91c8b056306240f5b8c9a21d2dace023",
      "0cbdd65f8e394f219dba5ebcb19be16e",
      "09ef2d90578e468f89e83e08117ba04f",
      "327d7434f260455da0a2c4d28150b878",
      "db3cc5625ea84fa6b559030753f1955b",
      "4d62bb9b174c415e8e7a97f77ea785ce",
      "06378bdf146146c7b373815bebd12696",
      "4d5def43d9514610ae285bcd54d4d7c6",
      "69431c2a5c1d4d8f8231810b1d6833e6",
      "105160c2a29441ccb9d7ddb59cc928c4",
      "6ea68135753b4a138171446c89e58410",
      "3951fa5a48894450bf4d1867a377fb20",
      "ac0bccfde44b41afa86365a517901fb9",
      "f3b231c52aa346968e2de84baa19b6a4",
      "e2a16391afb040549d67bb67d44b0ec5",
      "be5caf82d10d4b5eb51298ecd26e8d9e",
      "b2d747ee7f094555961d8ff67243e3fc",
      "308e2a3d58ba495fa8ca2e1e386c5bdf",
      "cca4f0e1e7454e44b3567aa0ead0f096",
      "7ef2ffec3e3e405ea56282ac8988cf89",
      "47b10a93c46443cea7813cfc897120ed",
      "c3fe7433dac144dcba17c482b7fce4ae",
      "9c4eec2e327e4f1ebde044c8defd4dc1",
      "7df61d2a727c49b99e5dacf81ffc2a9b",
      "ff7d4e76064b44ab9df3c4196144d1c3",
      "2ce41e27466845268186de9d7bbf9fcb",
      "60eaaf1148de4f288dd76cf3bd6d8c80",
      "7806c9b258484ab6bd908ebb334b2817",
      "da5e8cc186914a319ac79984617c9f3b",
      "fe1df35103f140b4a6c0c0982fffdc3a",
      "92ace0dc76d94141b702c94a8ffb763a",
      "1aa6eef3c25241ba8604e361b182239c",
      "bd312be740e54effbecb167e11915e47",
      "f7e0b53d264441bda9e3ea885ff7fc79",
      "68788cde7fce41e5960d81551053e12e",
      "3f5d787006154c4aa558f6915c788fe8",
      "fe1c84bd7576411eac556b0de32bd717",
      "7cb7600ab3eb40dea453b4f7978b78dc",
      "25950055960541f38b02c9eec1b24680",
      "c24c7b64447f4b78aa75f6498cd6f0af",
      "f6f2d6c7e4a24513b10eb562187c424b",
      "0122b1a88e944d7d94e7d3b7130dbd97",
      "59826a5116cc4b60965200402c1dd826",
      "d36e93fba28c4b6499a58ccb2ce6ba1b",
      "efa7fe32583e4271b4374f503cf06f56",
      "89fe24d2f83345eab639832231a91f9a",
      "eeb33b75449c4072a014095fba5a3ed6",
      "6897bd3ff7984accb8b599b4cf8424de",
      "fa031c5fdac24ea199126a58ab38d114",
      "3f5d98cc20dc4473b69f83b3ef4aae81",
      "ed57646c7a2642c497dd88307c502828",
      "1e7eb30f6f8844679d09e215babb3691",
      "00a0daedf42b4fef93140984d345cd59",
      "eaec66b27f3a4107931376ab745c90e9",
      "4e8b45a4d24d443fb5bfb733a119d525",
      "45bb437ee1f449b6b6e7bd42f2d58122",
      "09af91ff98f54317b7a927e10732c8c5",
      "897e4ab987c7455d90515d501a022265",
      "fde6def77b054d6193a051906c1a2b41"
     ]
    },
    "id": "yn6gXokJfZP7",
    "outputId": "99cee6cb-65a7-42a8-9d4b-4c24df9676d8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "783f934afc62497ab988eaea82aca007",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/614 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea88f60c993e490d93770b315c3a88fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a06c787e2142499daa5ee26f7dfcc66f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eea5b8e1ebcf4b0b85e1141d95608b4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7df0f0b1c60c4013b3e171aabce7f13a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf4afc8af1a94e9c977f187f03b28a6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a233654a3ff45d0bb119e4f9194517a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd405a7e80b94d3487f0884839ed9078",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.62k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f095a60bfb64c00ac3866117283567e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45463f1c44e14bdea35970c8ad65f57e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fb801b293f54e57acc125ef9fe9231e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "<ipython-input-4-7fc7f7e6e09a>:118: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
      "  llm = HuggingFacePipeline(pipeline=generate_text)\n",
      "<ipython-input-4-7fc7f7e6e09a>:154: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4b863b60e05432a9576a1cff78c58cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64783898633b4c9392700c21e8c5240d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5065a5bf61934180a4cab4ec30139910",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/94.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b904557d5da94906ac33e1ef8db077d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a72b129f0c824b66923fa1baa57794d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/777 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9aafaed66f740c2a96f0ab763e48e4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db3cc5625ea84fa6b559030753f1955b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be5caf82d10d4b5eb51298ecd26e8d9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60eaaf1148de4f288dd76cf3bd6d8c80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cb7600ab3eb40dea453b4f7978b78dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa031c5fdac24ea199126a58ab38d114",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-7fc7f7e6e09a>:202: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot = gr.Chatbot()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
      "\n",
      "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
      "* Running on public URL: https://665e404a214b89b44e.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://665e404a214b89b44e.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'chat_history = []\\n\\nwhile True:\\n    user_input = input(f\"Prompt: \")\\n\\n    if user_input.lower() == \\'exit\\':\\n        print(\\'Exiting\\')\\n        break\\n\\n    if user_input == \\'\\':\\n        continue\\n\\n    result = chain({\\'query\\': user_input, \"chat_history\": chat_history})\\n\\n    if \\'result\\' in result:\\n        print(f\"Answer: {result[\\'result\\']}\")\\n        chat_history.append((user_input, result[\"result\"]))\\n\\n        print(result[\\'source_documents\\'])\\n    else:\\n        print(\"No \\'result\\' key found in the response.\")'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import bitsandbytes\n",
    "\n",
    "import asyncio\n",
    "import bitsandbytes as bnb\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "from torch import cuda, bfloat16\n",
    "import transformers\n",
    "import torch\n",
    "from transformers import StoppingCriteria, StoppingCriteriaList,AutoConfig\n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "import torch\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "import gradio as gr\n",
    "\n",
    "'''from langchain import PromptTemplate'''\n",
    "\n",
    "from langchain.document_loaders import WebBaseLoader,MergedDataLoader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_id = 'meta-llama/Llama-2-7b-chat-hf'\n",
    "\n",
    "#device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "bnb_config = transformers.BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "hf_auth = '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'\n",
    "\n",
    "model_config = transformers.AutoConfig.from_pretrained(\n",
    "    model_id,\n",
    "    token=hf_auth  # Replace use_auth_token with token\n",
    ")\n",
    "\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    config=model_config,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='auto',\n",
    "    #load_in_4bit=True,\n",
    "    token=hf_auth  # Replace use_auth_token with token\n",
    ")\n",
    "\n",
    "\n",
    "# enable evaluation mode to allow model inference\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model loaded on {device}\")\n",
    "\n",
    "\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    token=hf_auth\n",
    "      #use_auth_token=hf_auth\n",
    ")\n",
    "\n",
    "\n",
    "stop_list = ['\\nHuman:', '\\n```\\n']\n",
    "\n",
    "stop_token_ids = [tokenizer(x)['input_ids'] for x in stop_list]\n",
    "stop_token_ids\n",
    "\n",
    "\n",
    "\n",
    "stop_token_ids = [torch.LongTensor(x).to(device) for x in stop_token_ids]\n",
    "stop_token_ids\n",
    "\n",
    "\n",
    "# define custom stopping criteria object\n",
    "class StopOnTokens(StoppingCriteria):\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        for stop_ids in stop_token_ids:\n",
    "            if torch.eq(input_ids[0][-len(stop_ids):], stop_ids).all():\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "stopping_criteria = StoppingCriteriaList([StopOnTokens()])\n",
    "\n",
    "generate_text = transformers.pipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=False,\n",
    "    task='text-generation',\n",
    "    stopping_criteria=stopping_criteria,\n",
    "\n",
    "    max_new_tokens=300,\n",
    "    repetition_penalty=1.1\n",
    ")\n",
    "\n",
    "\n",
    "\"\"\"test\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"the same test but Implementing HF Pipeline in LangChain\"\"\"\n",
    "\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=generate_text)\n",
    "\n",
    "# checking again that everything is working fine\n",
    "#llm(prompt=\"what is DAA solution\")\n",
    "\n",
    "\"\"\"Ingesting Data using Document Loader\"\"\"\n",
    "web_loader = WebBaseLoader([\"https://www.linkedin.com/pulse/llm-vs-lcm-la-prochaine-r%C3%A9volution-de-lia-en-christoph-pellkofer-qmmre/\"])\n",
    "web_docs = web_loader.load()\n",
    "\n",
    "# Charger les documents du PDF\n",
    "'''pdf_loader=DirectoryLoader('data/',\n",
    "                       glob=\"*.pdf\",\n",
    "                       loader_cls=PyPDFLoader)\n",
    "pdf_docs = pdf_loader.load()'''\n",
    "\n",
    "\n",
    "# Fusionner les deux listes de documents\n",
    "\"\"\"merged_loader = MergedDataLoader(loaders=[web_loader, pdf_loader])\n",
    "documents = merged_loader.load()\"\"\"\n",
    "\n",
    "\n",
    "'''loader=DirectoryLoader('data/',\n",
    "                       glob=\"*.pdf\",\n",
    "                       loader_cls=PyPDFLoader)'''\n",
    "\n",
    "\n",
    "documents=web_loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=10)\n",
    "all_splits = text_splitter.split_documents(documents)\n",
    "\n",
    "\n",
    "\n",
    "model_name = \"BAAI/bge-base-en-v1.5\"\n",
    "model_kwargs = {\"device\": \"cuda\"}\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)\n",
    "\n",
    "# storing embeddings in the vector store\n",
    "vectorstore = FAISS.from_documents(all_splits, embeddings)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "custom_prompt = PromptTemplate(\n",
    "    template=\"\"\"You are an AI assistant who answers questions only using the following context.\n",
    "If you cannot find the answer in the context, simply say: \"I don't know.\"\n",
    "Do not generate additional information, only answer the question.\n",
    "\n",
    "    Contexte:\n",
    "    {context}\n",
    "\n",
    "    Question: {question}\n",
    "    Réponse:\"\"\",\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "chain = RetrievalQA.from_chain_type(llm=llm,\n",
    "                                    chain_type='stuff',\n",
    "                                    retriever=vectorstore.as_retriever(search_kwargs={'k': 2}),\n",
    "                                    chain_type_kwargs={\"prompt\": custom_prompt},\n",
    "                                    return_source_documents=True)\n",
    "\n",
    "\n",
    "def chatbot_response(user_input):\n",
    "    if user_input.lower() == \"exit\":\n",
    "        return \"Exiting...\"\n",
    "    if user_input.strip() == \"\":\n",
    "        return \"Merci de poser une question !\"\n",
    "\n",
    "    result = chain({'query': user_input})\n",
    "    response = result['result'].strip()\n",
    "    if user_input in response:\n",
    "        response = response.replace(user_input, \"\").strip()\n",
    "\n",
    "    return response\n",
    "\n",
    "# Interface Gradio\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"# 💬 Chatbot LLaMA 2\")\n",
    "    chatbot = gr.Chatbot()\n",
    "    user_input = gr.Textbox(placeholder=\"Pose ta question ici...\")\n",
    "    submit_btn = gr.Button(\"Envoyer\")\n",
    "\n",
    "    def chat(user_input, history):\n",
    "        response = chatbot_response(user_input)\n",
    "        history.append((user_input, response))\n",
    "        return history, \"\"\n",
    "\n",
    "    submit_btn.click(chat, inputs=[user_input, chatbot], outputs=[chatbot, user_input])\n",
    "\n",
    "# Lancer l’interface\n",
    "demo.launch()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Supprime le prompt ou autres éléments indésirables si présents\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''chat_history = []\n",
    "\n",
    "while True:\n",
    "    user_input = input(f\"Prompt: \")\n",
    "\n",
    "    if user_input.lower() == 'exit':\n",
    "        print('Exiting')\n",
    "        break\n",
    "\n",
    "    if user_input == '':\n",
    "        continue\n",
    "\n",
    "    result = chain({'query': user_input, \"chat_history\": chat_history})\n",
    "\n",
    "    if 'result' in result:\n",
    "        print(f\"Answer: {result['result']}\")\n",
    "        chat_history.append((user_input, result[\"result\"]))\n",
    "\n",
    "        print(result['source_documents'])\n",
    "    else:\n",
    "        print(\"No 'result' key found in the response.\")'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "rMmqEySbi1cm",
    "outputId": "b6c804ec-4883-413d-df0c-7a65d5ad6f95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting streamlit\n",
      "  Downloading streamlit-1.44.1-py3-none-any.whl.metadata (8.9 kB)\n",
      "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
      "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
      "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\n",
      "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.1.8)\n",
      "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.0.2)\n",
      "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (24.2)\n",
      "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.2.2)\n",
      "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.1.0)\n",
      "Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.29.4)\n",
      "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (18.1.0)\n",
      "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.3)\n",
      "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (9.1.2)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.13.0)\n",
      "Collecting watchdog<7,>=2.1.5 (from streamlit)\n",
      "  Downloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl.metadata (44 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.44)\n",
      "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
      "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (3.1.6)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
      "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.33.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2025.1.31)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.24.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
      "Downloading streamlit-1.44.1-py3-none-any.whl (9.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.8/9.8 MB\u001b[0m \u001b[31m74.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m89.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl (79 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: watchdog, pydeck, streamlit\n",
      "Successfully installed pydeck-0.9.1 streamlit-1.44.1 watchdog-6.0.0\n"
     ]
    }
   ],
   "source": [
    "pip install streamlit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fuw7PM7eisGA"
   },
   "source": [
    "with streamlit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h0b4-dT0Qm4w"
   },
   "source": [
    "sans historique\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 911,
     "referenced_widgets": [
      "5f84b005673c487bad7775fd29bfb7fa",
      "d1503dfd005146b693cf9d10e7c6e25f",
      "4010b347dc244474850630efa77827d8",
      "30fddb6f99504afdbab0593e27e9e56d",
      "4b4ca6cddb9c44fab17387a52a0868df",
      "99044217b7a0412ba7ea97aaffab4a34",
      "2b5637e125464c6dbd8a9e7cfe6aef0e",
      "39f64742213f4c0c98de4b9d26677cab",
      "25d9fb07b68949c5939b31814fc137e9",
      "d1ef4b6dd4bf442d9201c10ce1e0487b",
      "e3615dc53a934865a9dbae23869ab660",
      "95d886e8e68f4358a0def0277f6803b7",
      "e0498be11b9d4eca8d727536e5ec7e19",
      "cd83a5083d484621819234599157c1fa",
      "ad51e06ddf0c4fbfb914a2ad62804a23",
      "97a4fe69f8ba463c82235a51f6475cc5",
      "719d192c949f414887f3a5536179cb9b",
      "39867aa1cbfe47bda71d1e092324b0e6",
      "5961768b5181491dbe52aeb9135ea38f",
      "6ffe3acf586f43b4b87bd37570c0f37a",
      "a93009d55bbb45e5b5597f10b31db35f",
      "2c4d4fdafc734f1f91acb90e249084bf",
      "c79ebacbef4842c79b551bd851dd00a3",
      "2a01cb35a9df4147bb1c69deed439fff",
      "66afd44d833d4daba5b77c3af86d17df",
      "c02f2b77d103424dac2ddcbe4daa9af2",
      "19642fcf8d9e48d496a56ff195d569a2",
      "7c444df9ea4649ec974effab21bebc6b",
      "dc51acf56f754a5d8a1c8aabca37072d",
      "5bb3067299ed46c699a8123592c07451",
      "6c18f41ae2d7403aad28dc13b5460abe",
      "2e186e08f8f4499485169ca1b66c1a20",
      "f1ea1d52307a40b5865a227e5a241d29",
      "85706a5b13aa445aac1d27942c7f9598",
      "049ed8d81cb144369ddd8ce323cc5d03",
      "970780b3b2e9417cb1585229d4be5930",
      "63e5338bcf9f4fc69b5e05ff19b6d0b0",
      "32c03c402c58489d948e35e822f361e9",
      "7714ab0fa8294d309f1fc0597db3fbe0",
      "ac60ee097d0c48699176b613fd8c409d",
      "bdd456cd4c724c8a9d0d65ddc0e13774",
      "ec0ecb4b784f4179a5c694d27c794bfd",
      "19a7d250c01847f7885f438ff8aeb5f5",
      "a385267cc86e458da8446eab4abbe00c",
      "fba2054a46da41e8ad648d00d12deaa1",
      "16390d13efd547abac5d0ff41b1a2da5",
      "d264428e7a4443aca1bcc6a77f2c072d",
      "65787ee58ba64fa8aecd74947b7f7326",
      "5b801e593c4a4af4b1bf9e4b177c4a38",
      "9c45e265cc2f47d393717eb4135cd34b",
      "3caf7fdf434e462fa569bf50dafcd4df",
      "9d91cf30d7f542de9c8fdaa1f9ca71f7",
      "06bf03319a40466faf59f01d692fb316",
      "ec8a7f3fc5d14a0cb8ae8d21806594db",
      "da27993fa3c149f183492172be0d6319",
      "3e92d23196e64f1497b6a3a6c9f89ad1",
      "9a8901064b61421a81576c2cd1c59911",
      "bc0745b93d444e8aa4005340ae2156ad",
      "0ff9f786d06946a58c2a6b2ef2cea25b",
      "dc188fc905c44380950652894f6f2b3f",
      "81f272cdf791435ab365d880adc6cbd7",
      "e7e989b6e29b45d4a416ec608e402bdd",
      "0eb241e28d804fa2ba62ee26561fc1ab",
      "34b2151401754a19b8508ec5e3bfa3d1",
      "9148a630c19d4b729aa1412ff412cefc",
      "190f1bc5ae9b4766a475f29768243e1f",
      "05b2bc89b41a4c639c7b096bad236da3",
      "7bddfbeab94b4472989c7285c3c6ec07",
      "fe382b3d19ea48409f9c2f704f0e8c17",
      "d11e1c2ff7e0444e9300116a45704eb8",
      "0b3ec7135c9c4a60aadc48c7c1e4f2b4",
      "aff3bf24fc9d4fc892930f1b6ada7117",
      "e732b938e0f24748bd9f196afad03942",
      "76cf573d5ec24ac39c0e75cf418808b6",
      "84b049c5860142a08f23741d8a65e9b2",
      "8914369307d841cc936b6fe7dd1e3b98",
      "7ad130bcbe484cba9ecdbf1921f0ebd1"
     ]
    },
    "id": "P7OrwXB6nfK3",
    "outputId": "772cc71d-0883-4fcf-aa1e-9814ce73f80c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f84b005673c487bad7775fd29bfb7fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.58k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95d886e8e68f4358a0def0277f6803b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c79ebacbef4842c79b551bd851dd00a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85706a5b13aa445aac1d27942c7f9598",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fba2054a46da41e8ad648d00d12deaa1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e92d23196e64f1497b6a3a6c9f89ad1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05b2bc89b41a4c639c7b096bad236da3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
      "\n",
      "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
      "* Running on public URL: https://7a636a2d6e65edbb6c.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://7a636a2d6e65edbb6c.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from transformers import pipeline\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from newspaper import Article\n",
    "import feedparser\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import torch\n",
    "import transformers\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "# ------------------- MODELES --------------------\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\", device=0)\n",
    "\n",
    "model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "hf_auth = \"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\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "bnb_config = transformers.BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    token=hf_auth\n",
    ")\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_id, token=hf_auth)\n",
    "\n",
    "stop_list = [\"\\nHuman:\", \"\\n```\\n\"]\n",
    "stop_token_ids = [torch.LongTensor(tokenizer(x)[\"input_ids\"]).to(device) for x in stop_list]\n",
    "\n",
    "class StopOnTokens(transformers.StoppingCriteria):\n",
    "    def __call__(self, input_ids, scores, **kwargs):\n",
    "        for stop_ids in stop_token_ids:\n",
    "            if torch.eq(input_ids[0][-len(stop_ids):], stop_ids).all():\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "stopping_criteria = transformers.StoppingCriteriaList([StopOnTokens()])\n",
    "\n",
    "generate_text = transformers.pipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=False,\n",
    "    task=\"text-generation\",\n",
    "    stopping_criteria=stopping_criteria,\n",
    "    max_new_tokens=300,\n",
    "    repetition_penalty=1.1\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=generate_text)\n",
    "\n",
    "# ------------------- HELPERS --------------------\n",
    "def truncate_text(text, max_tokens=1024):\n",
    "    tokens = text.split()\n",
    "    return \" \".join(tokens[:max_tokens])\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "def clean_summary(summary):\n",
    "    soup = BeautifulSoup(summary, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "def summarize_text(text):\n",
    "    text = clean_text(text)\n",
    "    text = truncate_text(text)\n",
    "    summary = summarizer(text, max_length=200, min_length=50, max_new_tokens=150, do_sample=False)\n",
    "    return summary[0][\"summary_text\"]\n",
    "\n",
    "def load_pdf(file_path):\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    documents = loader.load()\n",
    "    return \" \".join([doc.page_content for doc in documents])\n",
    "\n",
    "def fetch_article_from_url(url):\n",
    "    article = Article(url)\n",
    "    article.download()\n",
    "    article.parse()\n",
    "    return article.text\n",
    "\n",
    "def fetch_articles_from_rss(url):\n",
    "    feed = feedparser.parse(url)\n",
    "    return [clean_summary(entry.summary) for entry in feed.entries]\n",
    "\n",
    "# ------------------- TRAITEMENT --------------------\n",
    "def process_input(choice, text_input, file_input, url_input, rss_input):\n",
    "    if choice == \"1\":\n",
    "        content = text_input\n",
    "    elif choice == \"2\":\n",
    "        content = load_pdf(file_input.name)\n",
    "    elif choice == \"3\":\n",
    "        content = fetch_article_from_url(url_input)\n",
    "    elif choice == \"4\":\n",
    "        summaries = fetch_articles_from_rss(rss_input)\n",
    "        content = \"\\n\\n\".join(summaries)\n",
    "    else:\n",
    "        return \"\", None, None\n",
    "\n",
    "    summary = summarize_text(content)\n",
    "\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "    chunks = splitter.create_documents([content])\n",
    "\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-base-en-v1.5\", model_kwargs={\"device\": \"cuda\"})\n",
    "    vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "\n",
    "    prompt_template = PromptTemplate(\n",
    "        template=\"\"\"Tu es un assistant IA qui répond uniquement en utilisant le contexte suivant.\n",
    "Si tu ne trouves pas la réponse dans le contexte, dis simplement : \\\"Je ne sais pas\\\".\n",
    "\n",
    "Contexte:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Réponse:\"\"\",\n",
    "        input_variables=[\"context\", \"question\"]\n",
    "    )\n",
    "\n",
    "    rag_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        retriever=vectorstore.as_retriever(search_kwargs={\"k\": 2}),\n",
    "        chain_type=\"stuff\",\n",
    "        chain_type_kwargs={\"prompt\": prompt_template},\n",
    "        return_source_documents=True\n",
    "    )\n",
    "\n",
    "    return summary, rag_chain, content\n",
    "\n",
    "# ------------------- GRADIO UI --------------------\n",
    "with gr.Blocks(title=\"Outil Article + Chatbot RAG\") as demo:\n",
    "    gr.Markdown(\"## 🔍 Résumeur & Chatbot basé sur article\")\n",
    "\n",
    "    with gr.Tab(\"Entrée\"):\n",
    "        choice = gr.Radio(\n",
    "            choices=[(\"1. Coller le texte\", \"1\"),\n",
    "                     (\"2. Charger un PDF\", \"2\"),\n",
    "                     (\"3. URL d'article\", \"3\"),\n",
    "                     (\"4. Flux RSS\", \"4\")],\n",
    "            label=\"Source de contenu\", value=\"1\")\n",
    "\n",
    "        text_input = gr.Textbox(label=\"Collez ici le texte\", lines=10, visible=True)\n",
    "        file_input = gr.File(label=\"Chargez un PDF\", visible=False)\n",
    "        url_input = gr.Textbox(label=\"URL\", visible=False)\n",
    "        rss_input = gr.Textbox(label=\"URL RSS\", visible=False)\n",
    "        start_btn = gr.Button(\"Analyser l'article\")\n",
    "\n",
    "    with gr.Tab(\"Résumé\"):\n",
    "        summary_output = gr.Textbox(label=\"Résumé généré\", lines=10)\n",
    "\n",
    "    with gr.Tab(\"Chatbot\"):\n",
    "        chat_input = gr.Textbox(label=\"Posez une question sur l'article\")\n",
    "        chat_output = gr.Textbox(label=\"Réponse du chatbot\", lines=4)\n",
    "        chat_btn = gr.Button(\"Répondre\")\n",
    "\n",
    "    state_chain = gr.State()\n",
    "\n",
    "    def toggle_visibility(choice):\n",
    "        return [\n",
    "            gr.update(visible=choice == \"1\"),\n",
    "            gr.update(visible=choice == \"2\"),\n",
    "            gr.update(visible=choice == \"3\"),\n",
    "            gr.update(visible=choice == \"4\")\n",
    "        ]\n",
    "\n",
    "    choice.change(toggle_visibility, inputs=choice,\n",
    "                  outputs=[text_input, file_input, url_input, rss_input])\n",
    "\n",
    "    def on_start(choice, text_input, file_input, url_input, rss_input):\n",
    "        summary, chain, _ = process_input(choice, text_input, file_input, url_input, rss_input)\n",
    "        return summary, chain\n",
    "\n",
    "    start_btn.click(on_start,\n",
    "                    inputs=[choice, text_input, file_input, url_input, rss_input],\n",
    "                    outputs=[summary_output, state_chain])\n",
    "\n",
    "    def on_chat(user_q, chain):\n",
    "        if not chain:\n",
    "            return \"Veuillez d'abord charger un article.\"\n",
    "        res = chain({\"query\": user_q})\n",
    "        return res[\"result\"].strip()\n",
    "\n",
    "\n",
    "\n",
    "    chat_btn.click(on_chat, inputs=[chat_input, state_chain], outputs=[chat_output])\n",
    "\n",
    "# ------------------- LANCEMENT --------------------\n",
    "demo.launch()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vjlZm8ISAUlt"
   },
   "source": [
    "chatbot rag resumee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 445
    },
    "id": "aH_sXmwwuBSm",
    "outputId": "b05689cc-4ec7-4cb1-ef4d-b563143ce927"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-461695f79c75>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m )\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m model = transformers.AutoModelForCausalLM.from_pretrained(\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0mmodel_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    569\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig_class\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub_configs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"text_config\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m                 \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m             return model_class.from_pretrained(\n\u001b[0m\u001b[1;32m    572\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m             )\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0mold_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_default_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4378\u001b[0m         \u001b[0;31m# Prepare the full device map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4379\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdevice_map\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4380\u001b[0;31m             \u001b[0mdevice_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_device_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_memory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhf_quantizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_in_fp32_regex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4382\u001b[0m         \u001b[0;31m# Finalize model weight initialization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_get_device_map\u001b[0;34m(model, device_map, max_memory, hf_quantizer, torch_dtype, keep_in_fp32_regex)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhf_quantizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m             \u001b[0mhf_quantizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_environment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mdevice_map\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/quantizers/quantizer_bnb_4bit.py\u001b[0m in \u001b[0;36mvalidate_environment\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdevice_map_without_lm_head\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"disk\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdevice_map_without_lm_head\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m    105\u001b[0m                     \u001b[0;34m\"Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m                     \u001b[0;34m\"quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. "
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from transformers import pipeline\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from newspaper import Article\n",
    "import feedparser\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import torch\n",
    "import transformers\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "# ------------------- MODELES --------------------\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\", device=0)\n",
    "\n",
    "model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "hf_auth = \"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\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "bnb_config = transformers.BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    token=hf_auth\n",
    ")\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_id, token=hf_auth)\n",
    "\n",
    "stop_list = [\"\\nHuman:\", \"\\n```\\n\"]\n",
    "stop_token_ids = [torch.LongTensor(tokenizer(x)[\"input_ids\"]).to(device) for x in stop_list]\n",
    "\n",
    "class StopOnTokens(transformers.StoppingCriteria):\n",
    "    def __call__(self, input_ids, scores, **kwargs):\n",
    "        for stop_ids in stop_token_ids:\n",
    "            if torch.eq(input_ids[0][-len(stop_ids):], stop_ids).all():\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "stopping_criteria = transformers.StoppingCriteriaList([StopOnTokens()])\n",
    "\n",
    "generate_text = transformers.pipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=False,\n",
    "    task=\"text-generation\",\n",
    "    stopping_criteria=stopping_criteria,\n",
    "    max_new_tokens=300,\n",
    "    repetition_penalty=1.1\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=generate_text)\n",
    "\n",
    "# ------------------- HELPERS --------------------\n",
    "def truncate_text(text, max_tokens=1024):\n",
    "    tokens = text.split()\n",
    "    return \" \".join(tokens[:max_tokens])\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "def clean_summary(summary):\n",
    "    soup = BeautifulSoup(summary, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "def summarize_text(text):\n",
    "    text = clean_text(text)\n",
    "    text = truncate_text(text)\n",
    "    summary = summarizer(text, max_length=200, min_length=50, max_new_tokens=150, do_sample=False)\n",
    "    return summary[0][\"summary_text\"]\n",
    "\n",
    "def load_pdf(file_path):\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    documents = loader.load()\n",
    "    return \" \".join([doc.page_content for doc in documents])\n",
    "\n",
    "def fetch_article_from_url(url):\n",
    "    article = Article(url)\n",
    "    article.download()\n",
    "    article.parse()\n",
    "    return article.text\n",
    "\n",
    "def fetch_articles_from_rss(url):\n",
    "    feed = feedparser.parse(url)\n",
    "    return [clean_summary(entry.summary) for entry in feed.entries]\n",
    "\n",
    "# ------------------- TRAITEMENT --------------------\n",
    "def process_input(choice, text_input, file_input, url_input, rss_input):\n",
    "    if choice == \"1\":\n",
    "        content = text_input\n",
    "    elif choice == \"2\":\n",
    "        content = load_pdf(file_input.name)\n",
    "    elif choice == \"3\":\n",
    "        content = fetch_article_from_url(url_input)\n",
    "    elif choice == \"4\":\n",
    "        summaries = fetch_articles_from_rss(rss_input)\n",
    "        content = \"\\n\\n\".join(summaries)\n",
    "    else:\n",
    "        return \"\", None, None\n",
    "\n",
    "    summary = summarize_text(content)\n",
    "\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "    chunks = splitter.create_documents([content])\n",
    "\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-base-en-v1.5\", model_kwargs={\"device\": \"cuda\"})\n",
    "    vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "\n",
    "    prompt_template = PromptTemplate(\n",
    "        template=\"\"\"Tu es un assistant IA qui répond uniquement en utilisant le contexte suivant.\n",
    "Si tu ne trouves pas la réponse dans le contexte, dis simplement : \\\"Je ne sais pas\\\".\n",
    "\n",
    "\n",
    "\n",
    "Contexte:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Réponse:\"\"\",\n",
    "        input_variables=[\"context\", \"question\"]\n",
    "    )\n",
    "\n",
    "    rag_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        retriever=vectorstore.as_retriever(search_kwargs={\"k\": 2}),\n",
    "        chain_type=\"stuff\",\n",
    "        chain_type_kwargs={\"prompt\": prompt_template},\n",
    "        return_source_documents=True\n",
    "    )\n",
    "\n",
    "    return summary, rag_chain, content\n",
    "\n",
    "# ------------------- GRADIO UI --------------------\n",
    "with gr.Blocks(title=\"Outil Article + Chatbot RAG\") as demo:\n",
    "    gr.Markdown(\"## 🔍 Résumeur & Chatbot basé sur article\")\n",
    "\n",
    "    with gr.Tab(\"Entrée\"):\n",
    "        choice = gr.Radio(\n",
    "            choices=[(\"1. Coller le texte\", \"1\"),\n",
    "                     (\"2. Charger un PDF\", \"2\"),\n",
    "                     (\"3. URL d'article\", \"3\"),\n",
    "                     (\"4. Flux RSS\", \"4\")],\n",
    "            label=\"Source de contenu\", value=\"1\")\n",
    "\n",
    "        text_input = gr.Textbox(label=\"Collez ici le texte\", lines=10, visible=True)\n",
    "        file_input = gr.File(label=\"Chargez un PDF\", visible=False)\n",
    "        url_input = gr.Textbox(label=\"URL\", visible=False)\n",
    "        rss_input = gr.Textbox(label=\"URL RSS\", visible=False)\n",
    "        start_btn = gr.Button(\"Analyser l'article\")\n",
    "\n",
    "    with gr.Tab(\"Résumé\"):\n",
    "        summary_output = gr.Textbox(label=\"Résumé généré\", lines=10)\n",
    "\n",
    "    with gr.Tab(\"Chatbot\"):\n",
    "        chatbot = gr.Chatbot()\n",
    "        chat_input = gr.Textbox(label=\"Posez une question sur l'article\")\n",
    "        chat_btn = gr.Button(\"Répondre\")\n",
    "\n",
    "    state_chain = gr.State()\n",
    "\n",
    "    def toggle_visibility(choice):\n",
    "        return [\n",
    "            gr.update(visible=choice == \"1\"),\n",
    "            gr.update(visible=choice == \"2\"),\n",
    "            gr.update(visible=choice == \"3\"),\n",
    "            gr.update(visible=choice == \"4\")\n",
    "        ]\n",
    "\n",
    "    choice.change(toggle_visibility, inputs=choice,\n",
    "                  outputs=[text_input, file_input, url_input, rss_input])\n",
    "\n",
    "    def on_start(choice, text_input, file_input, url_input, rss_input):\n",
    "        summary, chain, _ = process_input(choice, text_input, file_input, url_input, rss_input)\n",
    "        return summary, chain\n",
    "\n",
    "    start_btn.click(on_start,\n",
    "                    inputs=[choice, text_input, file_input, url_input, rss_input],\n",
    "                    outputs=[summary_output, state_chain])\n",
    "\n",
    "    def on_chat(user_q, history, chain):\n",
    "        if not chain:\n",
    "            return \"Veuillez d'abord charger un article.\", history\n",
    "\n",
    "        res = chain({\"query\": user_q})\n",
    "        response = res[\"result\"].strip()\n",
    "\n",
    "        history.append((user_q, response))\n",
    "        return history, \"\"\n",
    "\n",
    "    chat_btn.click(on_chat, inputs=[chat_input, chatbot, state_chain], outputs=[chatbot, chat_input])\n",
    "\n",
    "# ------------------- LANCEMENT --------------------\n",
    "demo.launch()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iSIDouE0_1sZ"
   },
   "source": [
    "\n",
    "\n",
    "```\n",
    "# Ce texte est au format code\n",
    "```\n",
    "\n",
    "*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*\n",
    "\n",
    "> Ajouter une citation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "Rkj6m7U3U31u"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "IvN_dfnPWf71",
    "outputId": "fcbcf45f-6b98-4288-c505-b8d93ca41f24"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: bitsandbytes 0.45.4\n",
      "Uninstalling bitsandbytes-0.45.4:\n",
      "  Successfully uninstalled bitsandbytes-0.45.4\n",
      "Collecting bitsandbytes\n",
      "  Using cached bitsandbytes-0.45.4-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.0.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\n",
      "Using cached bitsandbytes-0.45.4-py3-none-manylinux_2_24_x86_64.whl (76.0 MB)\n",
      "Installing collected packages: bitsandbytes\n",
      "Successfully installed bitsandbytes-0.45.4\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y bitsandbytes\n",
    "!pip install -U bitsandbytes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 333,
     "referenced_widgets": [
      "55252c8f02b045c58e54423c89a69c7d",
      "178371fd8ac94c318442cf98cb274e48",
      "07ef92906d4b4e5a933fa7d422eb7e16",
      "5fef7624274e4df9b1b83ede909ec02b",
      "6671cb8e8197448aa2555e96eb9f5038",
      "e93257d2c1a24a0d9206b6def0532f2e",
      "f66c407510cd4755ba4c0a28e363fc61",
      "d68f2e35152b43dd9e92e4c3d6c80f57",
      "3c456b91a57c40d6ae852329db8aa032",
      "ce8cc3c8563f49d2ba74a9e9909ba292",
      "04a325c7f76d43c8908a754aaffd4c38",
      "93167f1c200c4051b863db8b2efde1f3",
      "3a9dc0279e0743d893d1d1a57ecc1529",
      "019b9e4fc5b84adb85a8dc8036781f02",
      "830807151c59455f9a6907e12441f81e",
      "6ba15f60d0b74a58ac347d6302d36206",
      "3f1e1da29e934500baf7a115f4729ee1",
      "2e4b4836fd4b45a19b28449dece68938",
      "134b58b416f24f0481c495d99beae99e",
      "f558037e76114e0baaba9e7cea86c4af",
      "03648dc197ba4b038a518bbe6137eb1b",
      "8ff11fd6ec404a08b93c8d408130457f",
      "9f76eea7276343ac9b8c2e8ba11f8d4f",
      "c33cd907a21147be92a2295c3bbfd79b",
      "c77343351cc04f3dbd936446fe939778",
      "ac42e03a7e0649e482f65b9c6d13ba3a",
      "1dc299c344aa4a75bad590918e1d4b92",
      "4e5fbcdebe42463b8b1999ea01281048",
      "f291dbc376924c3d87d186e20acbec94",
      "0b7f1c9d130f4a5397993981db369414",
      "d1851ffcf0a44558be1ec5f62298b4cf",
      "d387df1436614737a387680220d6e972",
      "e6a9886aa2ff44ab872d96a972a5ae7e",
      "0be4dc1c101e4ec9b6fb4d063a558e75",
      "812050c640474fbaaa3727698f21b577",
      "1e51b4c0beac4a24be213e5079622ee6",
      "993a76c042ea4d259d114ba9004c1cc6",
      "19442fa05d014581a2119dbdd0a73517",
      "474c4eb3c20e40febb56fc36741e602f",
      "f9555cde3a7f4e868cde6c88529197c0",
      "098da547343146c9956171f759e86b1d",
      "93d11b97c41b4291991e73c5f12e889b",
      "cecd2b62db8040abb00942c40b7e6ee1",
      "f83fcdb785ad4d72aea7a1f0612f256b",
      "33b7d0a6c7294f62bfabba5f6d50da61",
      "8c7ddfef8b124d00a6e6faf4b82cc4c1",
      "a40f13d98449445fa6ab53f7daeb1270",
      "0ef9d6123f61410f9302d11e623125d2",
      "7ee497abd8b14b88804dc14fa321ad8d",
      "e2d1571a59944e3995fb16bf633b4289",
      "664b1a2119dd497dbeabb80098a52adf",
      "9117550724614fd7804b5bb08d92a37c",
      "2778db12a0034f5190f7231cff8de43b",
      "a2e56d405ead4107a0805f5ecf9a8897",
      "99fb39b2b0954e35971a7b8a9417c2b9",
      "0977e34a7e8d4214824e3ac8172f0134",
      "3dc7d0ee523c430289d3e5ad71103cb0",
      "0b877a9b8f1d489b863ed3ee6b8012ce",
      "a956bc0a8abd48a68440bc2834c326ac",
      "6a3e449c45974a0aa30982b986ca0df1",
      "3947040de14944e582a4a55fd2707727",
      "0e88de3c2c404fc690a759e4ffcc20eb",
      "5cb877c346f045eeb6941b24525bf4e8",
      "d668fd87b5404deb8b420b5418050cff",
      "84f2b43323994876a7d32cc86a9a1cc6",
      "4056951c8097435b89327594e7107e6e"
     ]
    },
    "collapsed": true,
    "id": "18oeDlbXVLzr",
    "outputId": "911212a2-4187-454f-8a72-839c2bb3b17c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55252c8f02b045c58e54423c89a69c7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.58k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93167f1c200c4051b863db8b2efde1f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f76eea7276343ac9b8c2e8ba11f8d4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0be4dc1c101e4ec9b6fb4d063a558e75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33b7d0a6c7294f62bfabba5f6d50da61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0977e34a7e8d4214824e3ac8172f0134",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"facebook/bart-large-cnn\", device_map=\"auto\",quantization_config=quantization_config,)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KUZkF6xOAzFT"
   },
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"deepset/roberta-base-squad2\", device_map=\"auto\",quantization_config=quantization_config,)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"deepset/roberta-base-squad2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 403,
     "referenced_widgets": [
      "3c226442cbed480e9c04669740fd5f77",
      "ecd2397c59c9457eac499dbb206bdb1d",
      "763e5389f90243d9a325d3a4f5c6efee",
      "414e25dea69e4587a6d89ab48cde3247",
      "1f147bb4949d4f5fad972131a2df322f",
      "e7859adf30a04c0ea0a778ead28a82a3",
      "b7ee0cb61efc44f9b64c0f82217a98c4",
      "7bb614cd163243d69e85f948ec04b68e",
      "3e3ca73069d64114b11d52c9106a4a52",
      "bc82162e458f42d68955a6b4c262683b",
      "5284d4c05b9b44be88d601c9e716c986",
      "37c2ac6d1920436fa0dc76aab42008f1",
      "026de90e92fd4ad1a73d9edc4dfaedbc",
      "014665157111447095c40e2a36473b7a",
      "67c23562a3134c32997832ae9ac54bb2",
      "3b0825d0183247e5a560df9920a04c88",
      "c70095cd74be4714a3225a3e002c71e0",
      "280f0dccf95248738ea1dbc86f0a17af",
      "302fd944d4c84e6a84a75296fee3bb0d",
      "0790c5fe689f4c719c8ac4825386f915",
      "f44be20fc08a40d481de3fd6f6769c17",
      "daa01a3425334a77bcbaf71b10db48a2",
      "9c4c6bd6f22a4f4dbca851292bc15d4d",
      "f1d85e89dc9544e2982bae3457eb1396",
      "3213cf0460e047d9ab4904a4dae74e0b",
      "a6da18197b77402f85287aeeeecf1db2",
      "01b5dd363c4340379882560680abe6b3",
      "b492714328a04bfd850623e80e3f11d5",
      "a62728baea664a5e9a872fcf9aa24a36",
      "cffcf9bb3a434045887c47dc7afb4c36",
      "2485da63832046af9eee537ef8b9a0c5",
      "24e083bf8f864095a5c501cca4e367d6",
      "2f21f784132c497a9251a4519f9cfc58",
      "aed92fe30be1484680775a2732d81d79",
      "25f3e4ad426e4670a61d736410ffaa8d",
      "3f368ec8d26e4396b673099970b91608",
      "ed4b0951df674b0b9116ccb29ff00676",
      "32ed19079e1d4bdca2de5e49001c9566",
      "96da71939a44462fafa3b51956dc54b7",
      "70a13bb374d44d729452be91ee47c4ab",
      "5d7de0021ac14d4db3b2d9dd8456d10b",
      "6e19dcb20e504c12b180be05bdf8c1de",
      "7e12d209ccba4a158f726ee39241faa2",
      "ac3b429c58c049efb661f7c8a1181b39",
      "7a088d060b5a4f5e84265e4b04e6eac8",
      "7a8b72cf470a4bbfb7aeab748a515187",
      "6ea0c3ecdd434819b65a596417b09f59",
      "43d84eecf813492899d5c6537294a025",
      "1fcc3f5d50c747f3b8369997704aaabb",
      "363dc3b03e71446ea34e2c8291f45007",
      "5e71dab027d84fad80a3dd585e2cf4fb",
      "ec8705a703cd4dc091bad5a09dfd4f9f",
      "ab455b3a101e4b938c82d59df733549a",
      "b22603df0961405ca34090948604dd5b",
      "6c1d51e7fc4749b985feef2b7f330e5b",
      "9d26b503bf7b4da0a87564200edf485a",
      "c123e7ae19074c4bbce9bbfd03d5bb9f",
      "2a926a637da64b2794ba98f25422b691",
      "8545d449aeba41bd87b964909c0a9e1a",
      "f7321df464b748eab351adec8a41a907",
      "596ebae81f004d86b2b1533e36eb6b9d",
      "6f3f0ce9fc4140c8a775ca7aa0f4e289",
      "29ea5ad38c0e49bdad958665f8eaa744",
      "79406f73f4cb47478113edd12d63a6ef",
      "d408cf18bbe24fde8f00021c23a95736",
      "38650143c2e84686b28778f5c31449c4"
     ]
    },
    "collapsed": true,
    "id": "WLb-p-TEXHuL",
    "outputId": "2858b87a-d4fd-498d-bd9e-0da9463fa83a"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c226442cbed480e9c04669740fd5f77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37c2ac6d1920436fa0dc76aab42008f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/496M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "Some weights of RobertaForCausalLM were not initialized from the model checkpoint at deepset/roberta-base-squad2 and are newly initialized: ['lm_head.bias', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c4c6bd6f22a4f4dbca851292bc15d4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/79.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aed92fe30be1484680775a2732d81d79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a088d060b5a4f5e84265e4b04e6eac8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d26b503bf7b4da0a87564200edf485a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/772 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "<ipython-input-8-66ae80da7d78>:17: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
      "  llm = HuggingFacePipeline(pipeline=text_generator)\n",
      "<ipython-input-8-66ae80da7d78>:20: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
      "<ipython-input-8-66ae80da7d78>:29: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  qa_chain = LLMChain(llm=llm, prompt=chat_prompt, memory=memory)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "XRedbjmS_WTw",
    "outputId": "e1d95801-648a-486d-ab6b-051a9d9b14dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "pip install torch --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ls-_UiDvBT57",
    "outputId": "f9573241-5f4e-41c7-a151-ddf4a0ca072b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n"
     ]
    }
   ],
   "source": [
    "print(\"CUDA Available:\", torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 550
    },
    "collapsed": true,
    "id": "ZW0uzk_UR-ps",
    "outputId": "3ba9a0bc-f71b-4dca-af15-d540596c46b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating pipeline...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choose an option:\n",
      "1. Paste the article\n",
      "2. Load the article from a file\n",
      "3. Enter the URL of an article\n",
      "4. Collect articles from RSS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 1024, but your input_length is only 979. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=489)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating summary...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=300) and `max_length`(=1024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:1484: UserWarning: Unfeasible length constraints: `min_length` (500) is larger than the maximum possible length (301). Generation will stop at the defined maximum length. You should decrease the minimum length and/or increase the maximum length.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-e41cf2efd3db>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-e41cf2efd3db>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"4. Collect articles from RSS\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m     \u001b[0mchoice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter your choice (1, 2, 3 or 4): \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchoice\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"1\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 680
    },
    "id": "2ZV6pgkTs28n",
    "outputId": "94e95a66-1023-4651-fff5-495684c532bf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
      "\n",
      "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
      "* Running on public URL: https://2686c666868f662bda.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://2686c666868f662bda.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import LLMChain, ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "import gradio as gr\n",
    "from transformers import pipeline\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from newspaper import Article\n",
    "import feedparser\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "\n",
    "# Initialisation des modèles\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\", device=0)\n",
    "qa_pipeline = pipeline(\"question-answering\", model=\"deepset/roberta-base-squad2\", device=0)\n",
    "\n",
    "text_generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=512,\n",
    "    do_sample=True,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=text_generator)\n",
    "\n",
    "# Mémoire pour le chatbot\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
    "\n",
    "# Template pour le chatbot\n",
    "chat_prompt = PromptTemplate.from_template(\n",
    "    \"Tu es un assistant expert en résumés et questions-réponses. Réponds clairement et de manière concise. \\n\"\n",
    "    \"Chat history: {chat_history}\\nUtilisateur: {question}\\nAssistant: \"\n",
    ")\n",
    "\n",
    "# Chaîne LangChain pour le chatbot\n",
    "qa_chain = LLMChain(llm=llm, prompt=chat_prompt, memory=memory)\n",
    "\n",
    "# Fonction pour poser une question avec mémoire\n",
    "def ask_question(question):\n",
    "    return qa_chain.run(question=question)\n",
    "\n",
    "\n",
    "\n",
    "def truncate_text(text, max_tokens=1024):\n",
    "    # Split the text into tokens (words) and truncate if necessary\n",
    "    tokens = text.split()\n",
    "    if len(tokens) > max_tokens:\n",
    "        tokens = tokens[:max_tokens]\n",
    "        text = \" \".join(tokens)\n",
    "    return text\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "def clean_summary(summary):\n",
    "    soup = BeautifulSoup(summary, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "def summarize_text(text):\n",
    "    # Clean and truncate the text\n",
    "    text = clean_text(text)\n",
    "    text = truncate_text(text)\n",
    "\n",
    "    # Generate the summary\n",
    "    print(\"Generating summary...\")\n",
    "    summary = summarizer(\n",
    "        text,\n",
    "        max_length=200,  # Increase max_length\n",
    "        min_length=50,   # Increase min_length\n",
    "        max_new_tokens=150,  # Increase max_new_tokens\n",
    "        do_sample=False,\n",
    "    )\n",
    "    return summary[0][\"summary_text\"]\n",
    "\n",
    "\n",
    "\n",
    "def load_pdf(file_path):\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    documents = loader.load()\n",
    "    return \" \".join([doc.page_content for doc in documents])\n",
    "\n",
    "def fetch_article_from_url(url):\n",
    "    article = Article(url)\n",
    "    article.download()\n",
    "    article.parse()\n",
    "    return article.text\n",
    "\n",
    "def fetch_articles_from_rss(url):\n",
    "    feed = feedparser.parse(url)\n",
    "    return [{\n",
    "        \"title\": entry.title,\n",
    "        \"link\": entry.link,\n",
    "        \"summary\": clean_summary(entry.summary),\n",
    "        \"published\": entry.published\n",
    "    } for entry in feed.entries]\n",
    "\n",
    "\n",
    "\n",
    "def process_choice(choice, text_input, file_input, url_input, rss_input, question_input):\n",
    "    try:\n",
    "        if choice == \"1\":  # Coller le texte\n",
    "            article = text_input\n",
    "        elif choice == \"2\":  # Fichier PDF\n",
    "            article = load_pdf(file_input.name)\n",
    "        elif choice == \"3\":  # URL d'article\n",
    "            article = fetch_article_from_url(url_input)\n",
    "        elif choice == \"4\":  # Flux RSS\n",
    "            articles = fetch_articles_from_rss(rss_input)\n",
    "            articles_text = \"\\n\\n\".join(\n",
    "                f\"Titre: {a['title']}\\nLien: {a['link']}\\nRésumé: {a['summary']}\\nPublié le: {a['published']}\"\n",
    "                for a in articles\n",
    "            )\n",
    "            return articles_text, \"\"\n",
    "\n",
    "        summary = summarize_text(article)\n",
    "\n",
    "        if question_input:\n",
    "            answer = ask_question(article, question_input)\n",
    "            return summary, answer\n",
    "        return summary, \"\"\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Erreur: {str(e)}\", \"\"\n",
    "\n",
    "# Interface Gradio\n",
    "with gr.Blocks(title=\"Outil de Résumé d'Articles\") as demo:\n",
    "    gr.Markdown(\"##  Outil de Résumé d'Articles IA\")\n",
    "    gr.Markdown(\"Chargez un article (texte, PDF, URL ou flux RSS) et obtenez un résumé automatique.\")\n",
    "\n",
    "    with gr.Tab(\"Options d'Entrée\"):\n",
    "        choice = gr.Radio(\n",
    "            choices=[\n",
    "                (\"1. Coller le texte\", \"1\"),\n",
    "                (\"2. Charger un fichier PDF\", \"2\"),\n",
    "                (\"3. Entrer une URL d'article\", \"3\"),\n",
    "                (\"4. Flux RSS\", \"4\")\n",
    "            ],\n",
    "            label=\"Choisissez votre méthode d'entrée\",\n",
    "            value=\"1\"\n",
    "        )\n",
    "\n",
    "        with gr.Column(visible=True) as text_input_col:\n",
    "            text_input = gr.Textbox(label=\"Collez votre article ici\", lines=10)\n",
    "\n",
    "        with gr.Column(visible=False) as file_input_col:\n",
    "            file_input = gr.File(label=\"Téléchargez un fichier PDF\")\n",
    "\n",
    "        with gr.Column(visible=False) as url_input_col:\n",
    "            url_input = gr.Textbox(label=\"Entrez l'URL de l'article\")\n",
    "\n",
    "        with gr.Column(visible=False) as rss_input_col:\n",
    "            rss_input = gr.Textbox(label=\"Entrez l'URL du flux RSS\")\n",
    "\n",
    "        submit_btn = gr.Button(\"Générer le Résumé\")\n",
    "\n",
    "    with gr.Tab(\"Résultats\"):\n",
    "        summary_output = gr.Textbox(label=\"Résumé\", lines=10)\n",
    "\n",
    "    with gr.Tab(\"Réponse Question\"):\n",
    "        question_input = gr.Textbox(label=\"Posez une question sur l'article (optionnel)\")\n",
    "\n",
    "        answer_output = gr.Textbox(label=\"Réponse à votre question\", visible=True )\n",
    "        response_btn = gr.Button(\"Générer la reponse\")\n",
    "\n",
    "\n",
    "\n",
    "    # Gestion de la visibilité des composants\n",
    "    def toggle_inputs(choice):\n",
    "        return [\n",
    "            gr.Column(visible=choice == \"1\"),\n",
    "            gr.Column(visible=choice == \"2\"),\n",
    "            gr.Column(visible=choice == \"3\"),\n",
    "            gr.Column(visible=choice == \"4\")\n",
    "        ]\n",
    "\n",
    "    choice.change(\n",
    "        toggle_inputs,\n",
    "        inputs=choice,\n",
    "        outputs=[text_input_col, file_input_col, url_input_col, rss_input_col]\n",
    "    )\n",
    "\n",
    "    # Gestion de la soumission\n",
    "    def on_submit(choice, text_input, file_input, url_input, rss_input):\n",
    "        summary, _ = process_choice(choice, text_input, file_input, url_input, rss_input, None)\n",
    "        return summary\n",
    "\n",
    "    submit_btn.click(\n",
    "        on_submit,\n",
    "        inputs=[choice, text_input, file_input, url_input, rss_input],\n",
    "        outputs=[summary_output]\n",
    "    )\n",
    "\n",
    "    # Gestion de la soumission pour générer une réponse à une question\n",
    "    def on_submit_question(choice, text_input, file_input, url_input, rss_input, question_input):\n",
    "        _, answer = process_choice(choice, text_input, file_input, url_input, rss_input, question_input)\n",
    "        return answer\n",
    "\n",
    "    response_btn.click(\n",
    "        on_submit_question,\n",
    "        inputs=[choice, text_input, file_input, url_input, rss_input, question_input],\n",
    "        outputs=[answer_output]\n",
    "    )\n",
    "\n",
    "# Lancement de l'interface\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6MGUF3y8_Ks1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0mGX5h05-p7W"
   },
   "outputs": [],
   "source": [
    " import gradio as gr\n",
    "from transformers import pipeline\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from newspaper import Article\n",
    "import feedparser\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "\n",
    "# Initialisation des modèles\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\", device=0)\n",
    "qa_pipeline = pipeline(\"question-answering\", model=\"deepset/roberta-base-squad2\", device=0)\n",
    "\n",
    "def truncate_text(text, max_tokens=1024):\n",
    "    # Split the text into tokens (words) and truncate if necessary\n",
    "    tokens = text.split()\n",
    "    if len(tokens) > max_tokens:\n",
    "        tokens = tokens[:max_tokens]\n",
    "        text = \" \".join(tokens)\n",
    "    return text\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "def clean_summary(summary):\n",
    "    soup = BeautifulSoup(summary, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "def summarize_text(text):\n",
    "    # Clean and truncate the text\n",
    "    text = clean_text(text)\n",
    "    text = truncate_text(text)\n",
    "\n",
    "    # Generate the summary\n",
    "    print(\"Generating summary...\")\n",
    "    summary = summarizer(\n",
    "        text,\n",
    "        max_length=200,  # Increase max_length\n",
    "        min_length=50,   # Increase min_length\n",
    "        max_new_tokens=150,  # Increase max_new_tokens\n",
    "        do_sample=False,\n",
    "    )\n",
    "    return summary[0][\"summary_text\"]\n",
    "\n",
    "\n",
    "\n",
    "def load_pdf(file_path):\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    documents = loader.load()\n",
    "    return \" \".join([doc.page_content for doc in documents])\n",
    "\n",
    "def fetch_article_from_url(url):\n",
    "    article = Article(url)\n",
    "    article.download()\n",
    "    article.parse()\n",
    "    return article.text\n",
    "\n",
    "def fetch_articles_from_rss(url):\n",
    "    feed = feedparser.parse(url)\n",
    "    return [{\n",
    "        \"title\": entry.title,\n",
    "        \"link\": entry.link,\n",
    "        \"summary\": clean_summary(entry.summary),\n",
    "        \"published\": entry.published\n",
    "    } for entry in feed.entries]\n",
    "\n",
    "def ask_question(article, question):\n",
    "    try:\n",
    "        result = qa_pipeline(question=question, context=article)\n",
    "        return result[\"answer\"]\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "def process_choice(choice, text_input, file_input, url_input, rss_input, question_input):\n",
    "    try:\n",
    "        if choice == \"1\":  # Coller le texte\n",
    "            article = text_input\n",
    "        elif choice == \"2\":  # Fichier PDF\n",
    "            article = load_pdf(file_input.name)\n",
    "        elif choice == \"3\":  # URL d'article\n",
    "            article = fetch_article_from_url(url_input)\n",
    "        elif choice == \"4\":  # Flux RSS\n",
    "            articles = fetch_articles_from_rss(rss_input)\n",
    "            articles_text = \"\\n\\n\".join(\n",
    "                f\"Titre: {a['title']}\\nLien: {a['link']}\\nRésumé: {a['summary']}\\nPublié le: {a['published']}\"\n",
    "                for a in articles\n",
    "            )\n",
    "            return articles_text, \"\"\n",
    "\n",
    "        summary = summarize_text(article)\n",
    "\n",
    "        if question_input:\n",
    "            answer = ask_question(article, question_input)\n",
    "            return summary, answer\n",
    "        return summary, \"\"\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Erreur: {str(e)}\", \"\"\n",
    "\n",
    "# Interface Gradio\n",
    "with gr.Blocks(title=\"Outil de Résumé d'Articles\") as demo:\n",
    "    gr.Markdown(\"##  Outil de Résumé d'Articles IA\")\n",
    "    gr.Markdown(\"Chargez un article (texte, PDF, URL ou flux RSS) et obtenez un résumé automatique.\")\n",
    "\n",
    "    with gr.Tab(\"Options d'Entrée\"):\n",
    "        choice = gr.Radio(\n",
    "            choices=[\n",
    "                (\"1. Coller le texte\", \"1\"),\n",
    "                (\"2. Charger un fichier PDF\", \"2\"),\n",
    "                (\"3. Entrer une URL d'article\", \"3\"),\n",
    "                (\"4. Flux RSS\", \"4\")\n",
    "            ],\n",
    "            label=\"Choisissez votre méthode d'entrée\",\n",
    "            value=\"1\"\n",
    "        )\n",
    "\n",
    "        with gr.Column(visible=True) as text_input_col:\n",
    "            text_input = gr.Textbox(label=\"Collez votre article ici\", lines=10)\n",
    "\n",
    "        with gr.Column(visible=False) as file_input_col:\n",
    "            file_input = gr.File(label=\"Téléchargez un fichier PDF\")\n",
    "\n",
    "        with gr.Column(visible=False) as url_input_col:\n",
    "            url_input = gr.Textbox(label=\"Entrez l'URL de l'article\")\n",
    "\n",
    "        with gr.Column(visible=False) as rss_input_col:\n",
    "            rss_input = gr.Textbox(label=\"Entrez l'URL du flux RSS\")\n",
    "\n",
    "        submit_btn = gr.Button(\"Générer le Résumé\")\n",
    "\n",
    "    with gr.Tab(\"Résultats\"):\n",
    "        summary_output = gr.Textbox(label=\"Résumé\", lines=10)\n",
    "\n",
    "    with gr.Tab(\"Réponse Question\"):\n",
    "        question_input = gr.Textbox(label=\"Posez une question sur l'article (optionnel)\")\n",
    "\n",
    "        answer_output = gr.Textbox(label=\"Réponse à votre question\", visible=True )\n",
    "        response_btn = gr.Button(\"Générer la reponse\")\n",
    "\n",
    "\n",
    "\n",
    "    # Gestion de la visibilité des composants\n",
    "    def toggle_inputs(choice):\n",
    "        return [\n",
    "            gr.Column(visible=choice == \"1\"),\n",
    "            gr.Column(visible=choice == \"2\"),\n",
    "            gr.Column(visible=choice == \"3\"),\n",
    "            gr.Column(visible=choice == \"4\")\n",
    "        ]\n",
    "\n",
    "    choice.change(\n",
    "        toggle_inputs,\n",
    "        inputs=choice,\n",
    "        outputs=[text_input_col, file_input_col, url_input_col, rss_input_col]\n",
    "    )\n",
    "\n",
    "    # Gestion de la soumission\n",
    "    def on_submit(choice, text_input, file_input, url_input, rss_input):\n",
    "        summary, _ = process_choice(choice, text_input, file_input, url_input, rss_input, None)\n",
    "        return summary\n",
    "\n",
    "    submit_btn.click(\n",
    "        on_submit,\n",
    "        inputs=[choice, text_input, file_input, url_input, rss_input],\n",
    "        outputs=[summary_output]\n",
    "    )\n",
    " def on_submit_chat(user_input):\n",
    "        return ask_question(user_input)\n",
    "\n",
    "    submit_chat.click(\n",
    "        on_submit_chat,\n",
    "        inputs=[user_input],\n",
    "        outputs=[chat_output]\n",
    "    )\n",
    "\n",
    "# Lancer l'application\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 332
    },
    "id": "3J0BebFc2XXt",
    "outputId": "fcdeea56-1974-408d-98ca-6690a2a0d675"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-18bf08814031>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/cuda/memory.py\u001b[0m in \u001b[0;36mempty_cache\u001b[0;34m()\u001b[0m\n\u001b[1;32m    216\u001b[0m     \"\"\"\n\u001b[1;32m    217\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_emptyCache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 460
    },
    "id": "qsYhjPayp-yB",
    "outputId": "8bfeed05-6588-4185-e80e-fdd23b3c9410"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-e7f364a9cf33>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Charger le modèle de résumé\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0msummarizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"summarization\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"facebook/bart-large-cnn\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mqa_pipeline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"question-answering\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"deepset/roberta-base-squad2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/__init__.py\u001b[0m in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m   1178\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"processor\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mpipeline_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframework\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mframework\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/text2text_generation.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         self.check_model_type(\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, tokenizer, feature_extractor, image_processor, processor, modelcard, framework, task, args_parser, device, torch_dtype, binary_output, **kwargs)\u001b[0m\n\u001b[1;32m    986\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mhf_device_map\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m         ):\n\u001b[0;32m--> 988\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    989\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    990\u001b[0m         \u001b[0;31m# If the model can generate:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3710\u001b[0m                     \u001b[0;34m\" `dtype` by passing the correct `torch_dtype` argument.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3711\u001b[0m                 )\n\u001b[0;32m-> 3712\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3713\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3714\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhalf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1341\u001b[0m                     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1343\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1345\u001b[0m     def register_full_backward_pre_hook(\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    928\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m             \u001b[0mp_should_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1327\u001b[0m                         \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_to_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m                     )\n\u001b[0;32m-> 1329\u001b[0;31m                 return t.to(\n\u001b[0m\u001b[1;32m   1330\u001b[0m                     \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m                     \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from transformers import pipeline\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from newspaper import Article\n",
    "import feedparser\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Charger le modèle de résumé\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\", device=0)\n",
    "qa_pipeline = pipeline(\"question-answering\", model=\"deepset/roberta-base-squad2\", device=0)\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Nettoyer le texte des balises HTML et caractères spéciaux.\"\"\"\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "def summarize_text(text):\n",
    "    \"\"\"Générer un résumé d'un texte donné.\"\"\"\n",
    "    summary = summarizer(text, max_length=500, min_length=100, max_new_tokens=300, do_sample=False)\n",
    "    return summary[0][\"summary_text\"]\n",
    "\n",
    "def load_pdf(file_path):\n",
    "    \"\"\"Extraire le texte d'un fichier PDF.\"\"\"\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    documents = loader.load()\n",
    "    return \" \".join([doc.page_content for doc in documents])\n",
    "\n",
    "def fetch_article_from_url(url):\n",
    "    \"\"\"Extraire le contenu d'un article à partir d'une URL.\"\"\"\n",
    "    article = Article(url)\n",
    "    article.download()\n",
    "    article.parse()\n",
    "    return article.text\n",
    "\n",
    "def fetch_articles_from_rss(url):\n",
    "    \"\"\"Récupérer les articles d'un flux RSS.\"\"\"\n",
    "    feed = feedparser.parse(url)\n",
    "    return [{\"title\": entry.title, \"link\": entry.link, \"summary\": entry.summary} for entry in feed.entries]\n",
    "\n",
    "def ask_question(article, question):\n",
    "    \"\"\"Répondre à une question sur le contenu de l'article.\"\"\"\n",
    "    try:\n",
    "        result = qa_pipeline(question=question, context=article)\n",
    "        return result[\"answer\"]\n",
    "    except Exception as e:\n",
    "        return f\"Erreur : {e}\"\n",
    "\n",
    "# Interface Gradio\n",
    "def process_input(choice, text=None, file=None, url=None, rss_url=None, question=None):\n",
    "    \"\"\"Gérer les différentes entrées et retourner le résumé ou la réponse.\"\"\"\n",
    "    article = \"\"\n",
    "\n",
    "    if choice == \"Texte\":\n",
    "        article = text\n",
    "    elif choice == \"Fichier PDF\" and file:\n",
    "        article = load_pdf(file.name)\n",
    "    elif choice == \"URL d'article\" and url:\n",
    "        article = fetch_article_from_url(url)\n",
    "    elif choice == \"Flux RSS\" and rss_url:\n",
    "        articles = fetch_articles_from_rss(rss_url)\n",
    "        return f\"Articles trouvés: {len(articles)}\", \"\\n\".join([f\"{a['title']} - {a['link']}\" for a in articles])\n",
    "\n",
    "    if not article:\n",
    "        return \"Erreur\", \"Impossible de récupérer l'article.\"\n",
    "\n",
    "    summary = summarize_text(article)\n",
    "\n",
    "    if question:\n",
    "        answer = ask_question(article, question)\n",
    "        return summary, answer\n",
    "    return summary, \"Posez une question pour obtenir une réponse.\"\n",
    "\n",
    "# Interface avec Gradio\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"# Outil de Veille Technologique 📰🤖\")\n",
    "\n",
    "    choice = gr.Radio([\"Texte\", \"Fichier PDF\", \"URL d'article\", \"Flux RSS\"], label=\"Sélectionnez une source\")\n",
    "\n",
    "    text_input = gr.Textbox(label=\"Collez un texte\", visible=False)\n",
    "    file_input = gr.File(label=\"Téléversez un fichier PDF\", visible=False)\n",
    "    url_input = gr.Textbox(label=\"Entrez une URL\", visible=False)\n",
    "    rss_input = gr.Textbox(label=\"Entrez un lien RSS\", visible=False)\n",
    "\n",
    "    question_input = gr.Textbox(label=\"Posez une question sur l'article (optionnel)\")\n",
    "    summarize_button = gr.Button(\"Générer le résumé\")\n",
    "\n",
    "    summary_output = gr.Textbox(label=\"Résumé généré\")\n",
    "    answer_output = gr.Textbox(label=\"Réponse à la question\")\n",
    "\n",
    "    def update_inputs(choice):\n",
    "        \"\"\"Afficher uniquement les champs pertinents selon le choix.\"\"\"\n",
    "        return {\n",
    "            text_input: gr.update(visible=choice == \"Texte\"),\n",
    "            file_input: gr.update(visible=choice == \"Fichier PDF\"),\n",
    "            url_input: gr.update(visible=choice == \"URL d'article\"),\n",
    "            rss_input: gr.update(visible=choice == \"Flux RSS\"),\n",
    "        }\n",
    "\n",
    "    choice.change(update_inputs, inputs=[choice], outputs=[text_input, file_input, url_input, rss_input])\n",
    "    summarize_button.click(\n",
    "        process_input,\n",
    "        inputs=[choice, text_input, file_input, url_input, rss_input, question_input],\n",
    "        outputs=[summary_output, answer_output]\n",
    "    )\n",
    "\n",
    "demo.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F4f7_ZqkOTVU"
   },
   "outputs": [],
   "source": [
    "pip install feedparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-hkj8fcYOVSV"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EcZnScuQYvhi"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, pipeline ,AutoModelForCausalLM,BartForConditionalGeneration\n",
    "from huggingface_hub import login\n",
    "\n",
    "\n",
    "# Initialiser Flask\n",
    "\n",
    "login(\"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\")\n",
    "\n",
    "# Modèle et Tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained( \"facebook/bart-large-cnn\", device_map=\"auto\",quantization_config=quantization_config, )\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 550
    },
    "id": "JgWkBUQuLAqf",
    "outputId": "d386a864-5bb8-46af-cf1a-c6c29e8c42d2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:2105: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-f019263a447d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m The largest and most capable LLMs are generative pretrained transformers (GPTs). Modern models can be fine-tuned for specific tasks or guided by prompt engineering. These models acquire predictive power regarding syntax, semantics, and ontologies inherent in human language corpora, but they also inherit inaccuracies and biases present in the data they are trained in.\"\"\"\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0msummary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msummarize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Summary:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-f019263a447d>\u001b[0m in \u001b[0;36msummarize\u001b[0;34m(text, max_length)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# Generate summary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     outputs = model.generate(\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2222\u001b[0m             \u001b[0;31m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2223\u001b[0;31m             result = self._sample(\n\u001b[0m\u001b[1;32m   2224\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2225\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepared_logits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3210\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_prefill\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3211\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3212\u001b[0m                 \u001b[0mis_prefill\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3213\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/deprecation.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/mistral/modeling_mistral.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m         \u001b[0;31m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 843\u001b[0;31m         outputs = self.model(\n\u001b[0m\u001b[1;32m    844\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/mistral/modeling_mistral.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[1;32m    520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 522\u001b[0;31m             \u001b[0minputs_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    523\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0muse_cache\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mpast_key_values\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m         return F.embedding(\n\u001b[0m\u001b[1;32m    191\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2549\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2550\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2551\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)"
     ]
    }
   ],
   "source": [
    "def summarize(text, max_length=200):\n",
    "    # Construct prompt\n",
    "    prompt = f\"\"\"Summarize the following text concisely:\n",
    "\n",
    "{text}\n",
    "\n",
    "Summary:\"\"\"\n",
    "\n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True)\n",
    "\n",
    "    # Generate summary\n",
    "    outputs = model.generate(\n",
    "        inputs.input_ids,\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=1,\n",
    "        do_sample=False\n",
    "    )\n",
    "\n",
    "    # Decode summary - CORRECT SYNTAX\n",
    "\n",
    "# Example usage\n",
    "text = \"\"\"A large language model (LLM) is a type of machine learning model designed for natural language processing tasks such as language generation. LLMs are language models with many parameters, and are trained with self-supervised learning on a vast amount of text.\n",
    "\n",
    "The largest and most capable LLMs are generative pretrained transformers (GPTs). Modern models can be fine-tuned for specific tasks or guided by prompt engineering. These models acquire predictive power regarding syntax, semantics, and ontologies inherent in human language corpora, but they also inherit inaccuracies and biases present in the data they are trained in.\"\"\"\n",
    "\n",
    "summary = summarize(text)\n",
    "print(\"Summary:\", summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w_7lxQ42ErPW",
    "outputId": "67827114-0a9b-42ea-ce04-8b4b3f3bfd3d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "The model 'BartForCausalLM' is not supported for summarization. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'Qwen2AudioForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n",
      "Both `max_new_tokens` (=50) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Création du pipeline...\n",
      "Résumé en cours...\n",
      "Résumé généré.\n",
      "Summary:  A large language model (LLM) is a type of machine learning model designed for natural language processing tasks such as language generation. LLMs are language models with many parameters, and are trained with self-supervised learning on a vast amount of text.\n",
      "\n",
      "The largest and most capable LLMs are generative pretrained transformers (GPTs). Modern models can be fine-tuned for specific tasks or guided by prompt engineering.[1] These models acquire predictive power regarding syntax, semantics, and ontologies[2] inherent in human language corpora, but they also inherit inaccuracies and biases present in the data they are trained in.o both \" only still one bothte both still both still only only only S both stillo both still were past had bothte only only both still stillo only bothte still both both still one still botho both both botho only only\n"
     ]
    }
   ],
   "source": [
    "print(\"Création du pipeline...\")\n",
    "summarizing = pipeline(\n",
    "    \"summarization\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=50,\n",
    "    repetition_penalty=1.2\n",
    ")\n",
    "\n",
    "prompt = \"\"\"\n",
    "        Please provide a concise and clear summary of the following text.\n",
    "        The summary should capture the key points and main ideas\"\"\"\n",
    "\n",
    "#You are an expert summarizer. Provide a concise summary of the following text, focusing on the most important points.\n",
    "\n",
    "ARTICLE = \"\"\"A large language model (LLM) is a type of machine learning model designed for natural language processing tasks such as language generation. LLMs are language models with many parameters, and are trained with self-supervised learning on a vast amount of text.\n",
    "\n",
    "The largest and most capable LLMs are generative pretrained transformers (GPTs). Modern models can be fine-tuned for specific tasks or guided by prompt engineering.[1] These models acquire predictive power regarding syntax, semantics, and ontologies[2] inherent in human language corpora, but they also inherit inaccuracies and biases present in the data they are trained in.\"\"\"\n",
    "# Add the prompt to the article\n",
    "#input_text = prompt + \"\\n\\n\" + ARTICLE\n",
    "\n",
    "print(\"Résumé en cours...\")\n",
    "summary = summarizing(ARTICLE, max_length=50, min_length=10, do_sample=False)\n",
    "print(\"Résumé généré.\")\n",
    "\n",
    "print(\"Summary:\", summary[0][\"summary_text\"])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
